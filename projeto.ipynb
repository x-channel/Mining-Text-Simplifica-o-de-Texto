{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projeto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/projeto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP583IXGNeHC",
        "colab_type": "text"
      },
      "source": [
        "# Projeto de Simplificação de Texto\n",
        "\n",
        "Nas primeiras abordagens, houve uma tentativa de produzir uma rede neural que tivesse como entrada o *token frequence* e saida outro *token frequence*.\n",
        " Apesar disso poder ser considerado um resultado válido para a tentativa de melhorar a classificação de texto, não podemos considerar uma frase como um conjunto de palavras com a propriedade da comutatividade.\n",
        "  A ordem das palavras acaba importando muito para os humanos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptDnWb-pbJUb",
        "colab_type": "text"
      },
      "source": [
        "## importando bugingangas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh5apxspbJ0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import random\n",
        "#import pandas as pd\n",
        "from urllib import request as req\n",
        "from gensim.models import keyedvectors as kv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQFno4IJOzW3",
        "colab_type": "text"
      },
      "source": [
        "## Paper With Code\n",
        "\n",
        "~~Após um gole de sorte, eu acabo por encontrar essa maravilha~~ chamado de [Paper With Code](https://paperswithcode.com/sota/document-summarization-on-cnn-daily-mail), é um ~~bom~~ compilado de resultados cientificos sobre determinados problemas da computação. O primeiro artigo do link de cima, mostra uma solução com ROGUE-1 de 43.83 para o problema de sumarização de documentos como o *state of the art*.\n",
        "\n",
        "![paper with code](https://github.com/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/imagens/paperwithcode.png?raw=true)\n",
        "<center>paper with code</center>\n",
        "\n",
        "No artigo *Text Summarization with Pretrained Encoders* (LIU e LAPATA, 2019), os autores representam o texto como *Bert*, que é gerado por uma rede neural e é um caso especial do *word2vec*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzGa9dI5viX",
        "colab_type": "text"
      },
      "source": [
        "## Representação do Bert word2vec\n",
        "\n",
        "*Kyubyong* criou um preset do Bert, um dicionário onde cada palavra é representada por um vetor de 768 valores decimais.\n",
        "Visto algumas limitações das plataformas *Google Colab* e do *Github*, serão usadas apenas 300 instâncias do CNN, que gerou um dicionário reduzido de apenas 25mb.\n",
        "\n",
        "![Bert is Evil](https://github.com/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/imagens/audio-banner.jpg?raw=true)\n",
        "<center>Bert is Evil</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iap7h5R65zWn",
        "colab_type": "text"
      },
      "source": [
        "## Base de dados da Cable News Network\n",
        "\n",
        "A base de dados pode ser encontrado [nesse github da google](https://github.com/google-research-datasets/sentence-compression), no formato *Json*. Essa base de dados é formada com notícias da CNN com a primeira linha da noticia, com o título, com todos os bigramas possíveis e com informações do TAG. Porém no trabalho apenas será usado a primeira linha como entrada e o título como saída.\n",
        "\n",
        "![Json](https://github.com/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/imagens/Jasonf.jpg?raw=true)\n",
        "<center>Json</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Nkg503WqUi",
        "colab_type": "text"
      },
      "source": [
        "## Mesclar o bert com o json.\n",
        "\n",
        "Basicamente esse script carrega o vocabulário do dicionário bert, onde a partir da segunda linha, cada linha é uma palavra, seguida de sua representação vetorial com 768 valores.\n",
        "Eventualmente aparecem algumas palavras estranhas como \"##,\".\n",
        "Essa palavra significa somente que a palavra anterior termina com uma virgula.\n",
        "\n",
        "Depois de carregar o vocabulário, ele começa a ler as noticias do json.\n",
        "Ele segmenta uma noticia, a transformando em um dicionário do python, procura os textos alvos\n",
        "### jfk['graph']['sentence'] e jfk['headline'].\n",
        "\n",
        "Com os textos em mãos, ele cata as palavras de cada texto, criando uma matriz de tamanho 768*2 pela quantidade de linhas do subtitulo da noticia.\n",
        "\n",
        "Observar que para esse código rodar ele deve estar na seguinte pasta ~~por isso está comentado~~. Com o json nomeado *in.json*, com o bert nomeado *dicionariolongo.vec* e uma pasta vazia chamada de *instancias*. O script vai gerar um *dicionario.csv* e varias noticias dentro da pasta instancias. Infelizmente algumas noticias tem caracters que não permitem seu uso como nome do arquivo, por isso todas as noticias foram nomeadas de *arquivo_n.csv* ~~Outro motivo para você guardar seus dados em um único arquivo json~~.\n",
        "\n",
        ">O diretório do arquivo.\n",
        ">>in.json \\n\n",
        "dicionariolongo.vec \\n\n",
        "preprobert.py\n",
        ">>> instancias\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Uesl-rWdP1",
        "colab_type": "code",
        "outputId": "4ac20d80-647e-40a4-dd4a-4000b3511677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "#texto para matriz Bert\n",
        "\n",
        "#!/usr/bin/python\n",
        "# -*- coding: UTF-8 -*-\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import fileinput\n",
        "\n",
        "import codecs\n",
        "\n",
        "import os\n",
        "\n",
        "dici = {}\n",
        "fins = {}\n",
        "dico = {} #dicionario reduzido\n",
        "\n",
        "\n",
        "#carrega o dicionario bert\n",
        "with open(\"dicionariolongo.vec\", \"r\", encoding=\"utf-8\") as d:\n",
        "    for i in d:\n",
        "        if len(i) > 200: #isso aqui eh para tirar a primeira linha, altura x largura\n",
        "            try:\n",
        "                a = i.split()\n",
        "                if '##' in a[0]:\n",
        "                    fins[a[0][2:].lower()] = []\n",
        "                    for j in a[1:]:\n",
        "                        fins[a[0][2:].lower()].append(float(j))\n",
        "                elif (not a[0].lower() in dici) or a[0].islower():\n",
        "                    dici[a[0].lower()] = []\n",
        "                    for j in a[1:]:\n",
        "                        dici[a[0].lower()].append(float(j))\n",
        "            except ValueError:\n",
        "                print(\"problema com uma palavra\")\n",
        "\n",
        "\n",
        "\n",
        "with open(\"in.json\", 'rb') as f:\n",
        "    data = f.read()\n",
        "\n",
        "data = str(data)[2:-1]\n",
        "data = data.replace('\\\\n}', '\\\\n}-----')\n",
        "data = data.split('-----')\n",
        "\n",
        "blanckl = []\n",
        "for i in range(768):\n",
        "    blanckl.append('')\n",
        "\n",
        "instancias = 0\n",
        "for p in data[0:-1]:\n",
        "    entrada = []\n",
        "    saida = []\n",
        "    try:\n",
        "        jfk = json.loads(codecs.decode(p, 'unicode_escape'))\n",
        "        arquivo = jfk['graph']['sentence'].lower().replace(\"/\",\"\").replace(\"\\\\\",\"\")\n",
        "        lg = jfk['graph']['sentence'].lower().split()\n",
        "        st = jfk['headline'].lower().split()\n",
        "\n",
        "        for i in lg: #isso deveria ser uma funcao, addlist() #semTempo\n",
        "            if i in dico: #dicionario reduzido\n",
        "                entrada.append(dico[i])\n",
        "            elif i in dici:\n",
        "                dico[i] = dici[i]\n",
        "                entrada.append(dico[i])\n",
        "            else:\n",
        "                for j, jj in fins.items():\n",
        "                    if j in i[-len(j):]:\n",
        "                        i = i[:-len(j)]\n",
        "                        if i in dico: #assim ela poderia ser chamada aqui\n",
        "                            entrada.append(dico[i])\n",
        "                        elif i in dici:\n",
        "                            dico[i] = dici[i]\n",
        "                            entrada.append(dico[i])\n",
        "                        if j in dico:\n",
        "                            entrada.append(dico[j])\n",
        "                        else:\n",
        "                            dico[j] = fins[j]\n",
        "                            entrada.append(dico[j])\n",
        "                        break\n",
        "        for i in st: #isso deveria ser uma funcao, addlist() #semTempo\n",
        "            if i in dico: #dicionario reduzido\n",
        "                saida.append(dico[i])\n",
        "            elif i in dici:\n",
        "                dico[i] = dici[i]\n",
        "                saida.append(dico[i])\n",
        "            else:\n",
        "                for j, jj in fins.items():\n",
        "                    if j in i[-len(j):]:\n",
        "                        i = i[:-len(j)]\n",
        "                        if i in dico: #assim ela poderia ser chamada aqui\n",
        "                            saida.append(dico[i])\n",
        "                        elif i in dici:\n",
        "                            dico[i] = dici[i]\n",
        "                            saida.append(dico[i])\n",
        "                        if j in dico:\n",
        "                            saida.append(dico[j])\n",
        "                        else:\n",
        "                            dico[j] = fins[j]\n",
        "                            saida.append(dico[j])\n",
        "                        break\n",
        "    except ValueError:\n",
        "        print(\"Houve um Erro\")\n",
        "\n",
        "        \n",
        "    sv = os.getcwd() + '\\\\instancias\\\\arquivo_%i'%instancias + '.csv'\n",
        "    instancias += 1\n",
        "    try:\n",
        "        with open(sv, 'w', newline = '') as file:\n",
        "            writer = csv.writer(file)\n",
        "            tam = 0\n",
        "            while tam < len(entrada):\n",
        "                tm = []\n",
        "                #print(type(tm))\n",
        "                tm.extend(entrada[tam])\n",
        "                if tam < len(saida):\n",
        "                    tm.extend(saida[tam])\n",
        "                else:\n",
        "                    tm.extend(blanckl)\n",
        "                tam += 1\n",
        "                writer.writerow(tm)\n",
        "    except ValueError:\n",
        "        print(\"erro no json\")\n",
        "\n",
        "with open(\"dicionario.csv\", \"w\", newline = '') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for i, ii in dico.items():\n",
        "\ttm = []\n",
        "\ttm.append(i)\n",
        "\ttm.extend(ii)\n",
        "        writer.writerow(tm)\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#texto para matriz Bert\\n\\n#!/usr/bin/python\\n# -*- coding: UTF-8 -*-\\n\\nimport pandas as pd\\nimport csv\\n\\nimport nltk\\nimport numpy as np\\n\\nimport json\\nimport fileinput\\n\\nimport codecs\\n\\nimport os\\n\\ndici = {}\\nfins = {}\\ndico = {} #dicionario reduzido\\n\\n\\n#carrega o dicionario bert\\nwith open(\"dicionariolongo.vec\", \"r\", encoding=\"utf-8\") as d:\\n    for i in d:\\n        if len(i) > 200: #isso aqui eh para tirar a primeira linha, altura x largura\\n            try:\\n                a = i.split()\\n                if \\'##\\' in a[0]:\\n                    fins[a[0][2:].lower()] = []\\n                    for j in a[1:]:\\n                        fins[a[0][2:].lower()].append(float(j))\\n                elif (not a[0].lower() in dici) or a[0].islower():\\n                    dici[a[0].lower()] = []\\n                    for j in a[1:]:\\n                        dici[a[0].lower()].append(float(j))\\n            except ValueError:\\n                print(\"problema com uma palavra\")\\n\\n\\n\\nwith open(\"in.json\", \\'rb\\') as f:\\n    data = f.read()\\n\\ndata = str(data)[2:-1]\\ndata = data.replace(\\'\\\\n}\\', \\'\\\\n}-----\\')\\ndata = data.split(\\'-----\\')\\n\\nblanckl = []\\nfor i in range(768):\\n    blanckl.append(\\'\\')\\n\\ninstancias = 0\\nfor p in data[0:-1]:\\n    entrada = []\\n    saida = []\\n    try:\\n        jfk = json.loads(codecs.decode(p, \\'unicode_escape\\'))\\n        arquivo = jfk[\\'graph\\'][\\'sentence\\'].lower().replace(\"/\",\"\").replace(\"\\\\\",\"\")\\n        lg = jfk[\\'graph\\'][\\'sentence\\'].lower().split()\\n        st = jfk[\\'headline\\'].lower().split()\\n\\n        for i in lg: #isso deveria ser uma funcao, addlist() #semTempo\\n            if i in dico: #dicionario reduzido\\n                entrada.append(dico[i])\\n            elif i in dici:\\n                dico[i] = dici[i]\\n                entrada.append(dico[i])\\n            else:\\n                for j, jj in fins.items():\\n                    if j in i[-len(j):]:\\n                        i = i[:-len(j)]\\n                        if i in dico: #assim ela poderia ser chamada aqui\\n                            entrada.append(dico[i])\\n                        elif i in dici:\\n                            dico[i] = dici[i]\\n                            entrada.append(dico[i])\\n                        if j in dico:\\n                            entrada.append(dico[j])\\n                        else:\\n                            dico[j] = fins[j]\\n                            entrada.append(dico[j])\\n                        break\\n        for i in st: #isso deveria ser uma funcao, addlist() #semTempo\\n            if i in dico: #dicionario reduzido\\n                saida.append(dico[i])\\n            elif i in dici:\\n                dico[i] = dici[i]\\n                saida.append(dico[i])\\n            else:\\n                for j, jj in fins.items():\\n                    if j in i[-len(j):]:\\n                        i = i[:-len(j)]\\n                        if i in dico: #assim ela poderia ser chamada aqui\\n                            saida.append(dico[i])\\n                        elif i in dici:\\n                            dico[i] = dici[i]\\n                            saida.append(dico[i])\\n                        if j in dico:\\n                            saida.append(dico[j])\\n                        else:\\n                            dico[j] = fins[j]\\n                            saida.append(dico[j])\\n                        break\\n    except ValueError:\\n        print(\"Houve um Erro\")\\n\\n        \\n    sv = os.getcwd() + \\'\\\\instancias\\\\arquivo_%i\\'%instancias + \\'.csv\\'\\n    instancias += 1\\n    try:\\n        with open(sv, \\'w\\', newline = \\'\\') as file:\\n            writer = csv.writer(file)\\n            tam = 0\\n            while tam < len(entrada):\\n                tm = []\\n                #print(type(tm))\\n                tm.extend(entrada[tam])\\n                if tam < len(saida):\\n                    tm.extend(saida[tam])\\n                else:\\n                    tm.extend(blanckl)\\n                tam += 1\\n                writer.writerow(tm)\\n    except ValueError:\\n        print(\"erro no json\")\\n\\nwith open(\"dicionario.csv\", \"w\", newline = \\'\\') as file:\\n    writer = csv.writer(file)\\n    for i, ii in dico.items():\\n\\ttm = []\\n\\ttm.append(i)\\n\\ttm.extend(ii)\\n        writer.writerow(tm)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKjJH3pbaixJ",
        "colab_type": "text"
      },
      "source": [
        "## Abrindo uma noticia (bert) e convertendo para texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6WEsqOPajg1",
        "colab_type": "code",
        "outputId": "dd17d296-c0d3-4f00-a3d3-54cb5282d175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "# Abrindo o dicionario criado com o outro script\n",
        "dicurl = 'https://raw.githubusercontent.com/x-channel/Mining-Text-Simplifica-o-de-Texto/master/dataset/dicionario.csv'\n",
        "#dicloader = req.urlopen(dicurl)\n",
        "vetores = []\n",
        "palavras = []\n",
        "\n",
        "with req.urlopen(dicurl) as f:\n",
        "  meucsv = f.read().decode('charmap')\n",
        "  meucsv = meucsv.split('\\n')[:-1]\n",
        "  for i in meucsv:\n",
        "    j = i.replace('\\r','').split(',')\n",
        "    palavras.append(j[0])\n",
        "    vetores.append(j[1:])\n",
        "\n",
        "# colocando o dicionario no gensim.\n",
        "dicio = kv.Word2VecKeyedVectors(768)\n",
        "dicio.add(palavras, np.array(vetores).astype(float))\n",
        "\n",
        "#busca por um vetor parecido\n",
        "five = dicio.get_vector('five')\n",
        "friends = dicio.get_vector('friends')\n",
        "\n",
        "print(dicio.similar_by_vector(five, 1), dicio.similar_by_vector(friends, 1))\n",
        "\n",
        "#abrindo a noticia\n",
        "#nn = input(\"digite um numero entre 0 e 199: \")\n",
        "nn = 1\n",
        "noticia = 'https://raw.githubusercontent.com/x-channel/Mining-Text-Simplifica-o-de-Texto/master/dataset/noticias/arquivo_%i.csv'%nn\n",
        "\n",
        "pastaNoticias = 'https://raw.githubusercontent.com/x-channel/Mining-Text-Simplifica-o-de-Texto/master/dataset/noticias/%s_%i.csv'\n",
        "\n",
        "def abrirNoticia(urlnoticia):\n",
        "  sentence = []\n",
        "  head = []\n",
        "  with req.urlopen(urlnoticia) as f:\n",
        "    meucsv = f.read().decode('charmap')\n",
        "    meucsv = meucsv.split('\\n')[:-1]\n",
        "    for i in meucsv:\n",
        "      j = i.replace('\\r',',').replace(',,', ',0.0,').replace(',,', ',0.0,')[:-1]\n",
        "      j = j.split(',')\n",
        "      sentence.append(j[:768])\n",
        "      head.append(j[768:])\n",
        "  sentence = np.array(sentence).astype(float)\n",
        "  head = np.array(head).astype(float)\n",
        "  return sentence, head\n",
        "\n",
        "def abrirNoticias(urlfolder, total, nome = 'arquivo'):\n",
        "  primeiro = []\n",
        "  cabecalh = []\n",
        "  for i in range(total):\n",
        "    par = abrirNoticia(urlfolder%(nome,i))\n",
        "    primeiro.append(par[0])\n",
        "    cabecalh.append(par[1])\n",
        "  return np.array(primeiro),np.array(cabecalh)\n",
        "\n",
        "\n",
        "def vec2head(matriz, model, li):\n",
        "  head = []\n",
        "  for i in matriz:\n",
        "    j = model.similar_by_vector(i,1)\n",
        "    if j[0][1] > li:\n",
        "      head.append(j)\n",
        "  return head\n",
        "\n",
        "s, h = abrirNoticia(noticia)\n",
        "\n",
        "titulo = vec2head(h, dicio, 0.5)\n",
        "print (titulo)\n",
        "\n",
        "noticias,cabecalhos = abrirNoticias(pastaNoticias, 200)\n",
        "print(vec2head(noticias[1], dicio, 0.5))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('five', 1.0)] [('friends', 1.0)]\n",
            "[[('several', 1.0)], [('school', 1.0)], [('districts', 1.0)], [('hold', 1.0)], [('classes', 1.0)], [('on', 1.0)], [('president', 0.6351367831230164)], [(\"##'\", 0.9999998807907104)], [('day', 1.0000001192092896)], [('to', 1.0)], [('make', 1.0)], [('up', 1.0000001192092896)], [('for', 1.0)], [('days', 1.0)], [('missed', 1.0)]]\n",
            "[[('several', 1.0)], [('school', 1.0)], [('districts', 1.0)], [('in', 1.0)], [('hampton', 1.0)], [('roads', 1.0)], [('are', 0.9999999403953552)], [('holding', 1.0)], [('classes', 1.0)], [('this', 1.0000001192092896)], [('president', 0.6351367831230164)], [(\"##'\", 0.9999998807907104)], [('day', 1.0000001192092896)], [('to', 1.0)], [('make', 1.0)], [('up', 1.0000001192092896)], [('for', 1.0)], [('days', 1.0)], [('missed', 1.0)], [('because', 1.0)], [('of', 1.0)], [('the', 1.0)], [(\"##'\", 0.696808934211731)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNJpRDlg4kQW",
        "colab_type": "text"
      },
      "source": [
        "## Dividindo a base de dados\n",
        "\n",
        "Aqui a base de dados é dividida em: Treinamento, validação e teste.\n",
        "\n",
        "OBS: não encontrei isso implementado nem no scikit learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AorJ_zzpsZIO",
        "colab_type": "code",
        "outputId": "f04a5a8e-1f68-47ce-9660-7a702c883bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#pading, para tudo ter o mesmo tamanho\n",
        "\n",
        "maxlen = 0\n",
        "\n",
        "for i in noticias:\n",
        "  if i.shape[0] > maxlen:\n",
        "    maxlen = i.shape[0]\n",
        "\n",
        "for i in range(len(noticias)):\n",
        "  pad = np.zeros((maxlen, 768))\n",
        "  pad[:noticias[i].shape[0],:] = noticias[i]\n",
        "  noticias[i] = pad\n",
        "  pad = np.zeros((maxlen, 768))\n",
        "  pad[:cabecalhos[i].shape[0],:] = cabecalhos[i]\n",
        "  cabecalhos[i] = pad\n",
        "\n",
        "print(maxlen)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9psE79f5CmX",
        "colab_type": "code",
        "outputId": "283f3c18-a4f2-4c4a-c026-2201bd89125f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "treinamento = []\n",
        "#validacao = []\n",
        "teste = []\n",
        "\n",
        "xt = []\n",
        "yt = []\n",
        "\n",
        "xr = []\n",
        "yr = []\n",
        "\n",
        "atreino = 0.9\n",
        "#avalidacao = 0.3\n",
        "\n",
        "d = []\n",
        "for i in range(len(noticias)):\n",
        "  d.append(i)\n",
        "\n",
        "random.shuffle(d)\n",
        "\n",
        "# deixar essa linha para validar os parametros dos primeiros testes\n",
        "d = [117, 135, 181, 2, 129, 167, 65, 183, 107, 104, 158, 111, 69, 194, 8, 101, 21, 35, 31, 188, 106, 196, 148, 198, 67, 60, 102, 82, 16, 88, 119, 61, 11, 115, 113, 56, 169, 98, 64, 40, 49, 162, 36, 127, 157, 66, 164, 180, 41, 138, 62, 34, 72, 178, 27, 189, 121, 154, 96, 14, 133, 145, 97, 43, 199, 51, 25, 163, 155, 47, 70, 150, 12, 30, 123, 195, 32, 55, 18, 176, 171, 68, 175, 120, 110, 59, 141, 6, 23, 44, 103, 151, 125, 130, 79, 73, 173, 1, 58, 165, 118, 46, 39, 191, 10, 74, 166, 24, 147, 131, 190, 20, 156, 26, 22, 187, 182, 75, 63, 52, 9, 132, 87, 5, 144, 192, 42, 142, 90, 85, 143, 13, 153, 174, 122, 139, 184, 128, 19, 50, 161, 172, 168, 83, 48, 71, 185, 53, 126, 4, 29, 86, 15, 7, 92, 45, 197, 76, 134, 37, 54, 152, 57, 84, 112, 3, 28, 93, 0, 109, 136, 177, 77, 170, 100, 146, 137, 179, 80, 33, 17, 124, 89, 193, 38, 160, 78, 95, 140, 114, 159, 81, 186, 99, 108, 105, 116, 94, 149, 91]\n",
        "\n",
        "for i in range(len(noticias)):\n",
        "  if i < atreino*len(noticias):\n",
        "    xt.append(noticias[d[i]])\n",
        "    yt.append(cabecalhos[d[i]])\n",
        "  else:\n",
        "    xr.append(noticias[d[i]])\n",
        "    yr.append(cabecalhos[d[i]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "xt = np.array(xt)\n",
        "yt = np.array(yt)\n",
        "\n",
        "xr = np.array(xr)\n",
        "yr = np.array(yr)\n",
        "\n",
        "xt = xt.reshape(xt.shape[0], maxlen, 768, 1)\n",
        "yt = yt.reshape(yt.shape[0], maxlen, 768, 1)\n",
        "\n",
        "xr = xr.reshape(xr.shape[0], maxlen, 768, 1)\n",
        "yr = yr.reshape(yr.shape[0], maxlen, 768, 1)\n",
        "\n",
        "print(len(xt), len(yt))\n",
        "print(len(xr), len(xr[1]), len(xr[1][1]))\n",
        "\n",
        "'''\n",
        "for i in range(len(noticias)):\n",
        "  if i < atreino*len(noticias):\n",
        "    treinamento.append(noticias[d[i]])\n",
        "  elif i < (atreino+avaliacao)*len(noticias):\n",
        "    validacao.append(noticias[d[i]])\n",
        "  else:\n",
        "    teste.append(noticias[d[i]])'''\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "180 180\n",
            "20 56 768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor i in range(len(noticias)):\\n  if i < atreino*len(noticias):\\n    treinamento.append(noticias[d[i]])\\n  elif i < (atreino+avaliacao)*len(noticias):\\n    validacao.append(noticias[d[i]])\\n  else:\\n    teste.append(noticias[d[i]])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7aMectc4YIn",
        "colab_type": "text"
      },
      "source": [
        "## Construindo a Rede Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx1l-6QCQCSY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "700e7afd-255b-4631-9614-b10a00d67223"
      },
      "source": [
        "#Importando o keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Deconvolution2D, Reshape\n",
        "import keras"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thAFO7kN4eFU",
        "colab_type": "code",
        "outputId": "5829ba8a-9fc8-4606-99d1-e271805752c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "model = Sequential()\n",
        "#model.add(Conv2D(2, kernel_size=(3,3), activation = \"tanh\", input_shape=(maxlen,768,1)))\n",
        "con = Conv2D(2, kernel_size=(3,3), activation = \"tanh\", input_shape=(maxlen,768,1),padding=\"same\")\n",
        "model.add(con)\n",
        "print(con.input_shape)\n",
        "print(con.output_shape)\n",
        "model.add(Conv2D(4, (2,2), activation = \"tanh\",padding=\"same\"))\n",
        "print(model.output_shape)\n",
        "#model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "#print(model.output_shape)\n",
        "\n",
        "print()\n",
        "model.add(Deconvolution2D(1, (2,2),padding=\"same\"))\n",
        "print(model.output_shape)\n",
        "\n",
        "#model.add(Reshape((maxlen,768,1)))\n",
        "#print(model.output_shape)\n",
        "\n",
        "#model.add(Conv2DTranspose(1,(2,2),output_shape=(768,), activation=\"tanh\"))\n",
        "#print(model.output_shape)\n",
        "#rs = Reshape((maxlen,768,1))\n",
        "#print(rs.output_shape)\n",
        "#print(rs.input_shape)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',loss='mse',metrics=[\"acc\"])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "(None, 56, 768, 1)\n",
            "(None, 56, 768, 2)\n",
            "(None, 56, 768, 4)\n",
            "\n",
            "(None, 56, 768, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnC1bMvV4eme",
        "colab_type": "text"
      },
      "source": [
        "## Treinando a rede"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyMsp1u95Jo2",
        "colab_type": "code",
        "outputId": "9a6b56d3-2b42-484d-d14b-b9a6e18fff14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(xt,yt,epochs=100)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "180/180 [==============================] - 7s 40ms/step - loss: 5.0080e-04 - acc: 0.8577\n",
            "Epoch 2/100\n",
            "180/180 [==============================] - 0s 763us/step - loss: 4.4486e-04 - acc: 0.8577\n",
            "Epoch 3/100\n",
            "180/180 [==============================] - 0s 789us/step - loss: 4.0066e-04 - acc: 0.8577\n",
            "Epoch 4/100\n",
            "180/180 [==============================] - 0s 752us/step - loss: 3.6607e-04 - acc: 0.8577\n",
            "Epoch 5/100\n",
            "180/180 [==============================] - 0s 733us/step - loss: 3.3802e-04 - acc: 0.8577\n",
            "Epoch 6/100\n",
            "180/180 [==============================] - 0s 768us/step - loss: 3.1646e-04 - acc: 0.8577\n",
            "Epoch 7/100\n",
            "180/180 [==============================] - 0s 769us/step - loss: 2.9915e-04 - acc: 0.8577\n",
            "Epoch 8/100\n",
            "180/180 [==============================] - 0s 860us/step - loss: 2.8524e-04 - acc: 0.8577\n",
            "Epoch 9/100\n",
            "180/180 [==============================] - 0s 808us/step - loss: 2.7420e-04 - acc: 0.8577\n",
            "Epoch 10/100\n",
            "180/180 [==============================] - 0s 778us/step - loss: 2.6520e-04 - acc: 0.8577\n",
            "Epoch 11/100\n",
            "180/180 [==============================] - 0s 760us/step - loss: 2.5780e-04 - acc: 0.8577\n",
            "Epoch 12/100\n",
            "180/180 [==============================] - 0s 792us/step - loss: 2.5176e-04 - acc: 0.8577\n",
            "Epoch 13/100\n",
            "180/180 [==============================] - 0s 835us/step - loss: 2.4697e-04 - acc: 0.8577\n",
            "Epoch 14/100\n",
            "180/180 [==============================] - 0s 800us/step - loss: 2.4298e-04 - acc: 0.8577\n",
            "Epoch 15/100\n",
            "180/180 [==============================] - 0s 843us/step - loss: 2.3973e-04 - acc: 0.8577\n",
            "Epoch 16/100\n",
            "180/180 [==============================] - 0s 848us/step - loss: 2.3715e-04 - acc: 0.8577\n",
            "Epoch 17/100\n",
            "180/180 [==============================] - 0s 783us/step - loss: 2.3503e-04 - acc: 0.8577\n",
            "Epoch 18/100\n",
            "180/180 [==============================] - 0s 766us/step - loss: 2.3332e-04 - acc: 0.8577\n",
            "Epoch 19/100\n",
            "180/180 [==============================] - 0s 815us/step - loss: 2.3199e-04 - acc: 0.8577\n",
            "Epoch 20/100\n",
            "180/180 [==============================] - 0s 803us/step - loss: 2.3087e-04 - acc: 0.8577\n",
            "Epoch 21/100\n",
            "180/180 [==============================] - 0s 812us/step - loss: 2.2999e-04 - acc: 0.8577\n",
            "Epoch 22/100\n",
            "180/180 [==============================] - 0s 831us/step - loss: 2.2926e-04 - acc: 0.8577\n",
            "Epoch 23/100\n",
            "180/180 [==============================] - 0s 802us/step - loss: 2.2868e-04 - acc: 0.8577\n",
            "Epoch 24/100\n",
            "180/180 [==============================] - 0s 802us/step - loss: 2.2819e-04 - acc: 0.8577\n",
            "Epoch 25/100\n",
            "180/180 [==============================] - 0s 808us/step - loss: 2.2780e-04 - acc: 0.8577\n",
            "Epoch 26/100\n",
            "180/180 [==============================] - 0s 818us/step - loss: 2.2742e-04 - acc: 0.8577\n",
            "Epoch 27/100\n",
            "180/180 [==============================] - 0s 813us/step - loss: 2.2713e-04 - acc: 0.8577\n",
            "Epoch 28/100\n",
            "180/180 [==============================] - 0s 825us/step - loss: 2.2686e-04 - acc: 0.8577\n",
            "Epoch 29/100\n",
            "180/180 [==============================] - 0s 860us/step - loss: 2.2666e-04 - acc: 0.8577\n",
            "Epoch 30/100\n",
            "180/180 [==============================] - 0s 821us/step - loss: 2.2644e-04 - acc: 0.8577\n",
            "Epoch 31/100\n",
            "180/180 [==============================] - 0s 785us/step - loss: 2.2626e-04 - acc: 0.8577\n",
            "Epoch 32/100\n",
            "180/180 [==============================] - 0s 769us/step - loss: 2.2606e-04 - acc: 0.8577\n",
            "Epoch 33/100\n",
            "180/180 [==============================] - 0s 769us/step - loss: 2.2594e-04 - acc: 0.8577\n",
            "Epoch 34/100\n",
            "180/180 [==============================] - 0s 769us/step - loss: 2.2583e-04 - acc: 0.8577\n",
            "Epoch 35/100\n",
            "180/180 [==============================] - 0s 854us/step - loss: 2.2565e-04 - acc: 0.8577\n",
            "Epoch 36/100\n",
            "180/180 [==============================] - 0s 906us/step - loss: 2.2553e-04 - acc: 0.8577\n",
            "Epoch 37/100\n",
            "180/180 [==============================] - 0s 812us/step - loss: 2.2544e-04 - acc: 0.8577\n",
            "Epoch 38/100\n",
            "180/180 [==============================] - 0s 777us/step - loss: 2.2532e-04 - acc: 0.8577\n",
            "Epoch 39/100\n",
            "180/180 [==============================] - 0s 815us/step - loss: 2.2522e-04 - acc: 0.8577\n",
            "Epoch 40/100\n",
            "180/180 [==============================] - 0s 771us/step - loss: 2.2508e-04 - acc: 0.8577\n",
            "Epoch 41/100\n",
            "180/180 [==============================] - 0s 812us/step - loss: 2.2501e-04 - acc: 0.8577\n",
            "Epoch 42/100\n",
            "180/180 [==============================] - 0s 834us/step - loss: 2.2491e-04 - acc: 0.8577\n",
            "Epoch 43/100\n",
            "180/180 [==============================] - 0s 834us/step - loss: 2.2482e-04 - acc: 0.8577\n",
            "Epoch 44/100\n",
            "180/180 [==============================] - 0s 818us/step - loss: 2.2475e-04 - acc: 0.8577\n",
            "Epoch 45/100\n",
            "180/180 [==============================] - 0s 832us/step - loss: 2.2467e-04 - acc: 0.8577\n",
            "Epoch 46/100\n",
            "180/180 [==============================] - 0s 802us/step - loss: 2.2460e-04 - acc: 0.8577\n",
            "Epoch 47/100\n",
            "180/180 [==============================] - 0s 830us/step - loss: 2.2453e-04 - acc: 0.8577\n",
            "Epoch 48/100\n",
            "180/180 [==============================] - 0s 813us/step - loss: 2.2447e-04 - acc: 0.8577\n",
            "Epoch 49/100\n",
            "180/180 [==============================] - 0s 849us/step - loss: 2.2440e-04 - acc: 0.8577\n",
            "Epoch 50/100\n",
            "180/180 [==============================] - 0s 801us/step - loss: 2.2435e-04 - acc: 0.8577\n",
            "Epoch 51/100\n",
            "180/180 [==============================] - 0s 822us/step - loss: 2.2429e-04 - acc: 0.8577\n",
            "Epoch 52/100\n",
            "180/180 [==============================] - 0s 809us/step - loss: 2.2423e-04 - acc: 0.8577\n",
            "Epoch 53/100\n",
            "180/180 [==============================] - 0s 819us/step - loss: 2.2417e-04 - acc: 0.8577\n",
            "Epoch 54/100\n",
            "180/180 [==============================] - 0s 817us/step - loss: 2.2413e-04 - acc: 0.8577\n",
            "Epoch 55/100\n",
            "180/180 [==============================] - 0s 837us/step - loss: 2.2410e-04 - acc: 0.8577\n",
            "Epoch 56/100\n",
            "180/180 [==============================] - 0s 833us/step - loss: 2.2403e-04 - acc: 0.8577\n",
            "Epoch 57/100\n",
            "180/180 [==============================] - 0s 844us/step - loss: 2.2401e-04 - acc: 0.8577\n",
            "Epoch 58/100\n",
            "180/180 [==============================] - 0s 800us/step - loss: 2.2396e-04 - acc: 0.8577\n",
            "Epoch 59/100\n",
            "180/180 [==============================] - 0s 826us/step - loss: 2.2390e-04 - acc: 0.8577\n",
            "Epoch 60/100\n",
            "180/180 [==============================] - 0s 771us/step - loss: 2.2388e-04 - acc: 0.8577\n",
            "Epoch 61/100\n",
            "180/180 [==============================] - 0s 820us/step - loss: 2.2384e-04 - acc: 0.8577\n",
            "Epoch 62/100\n",
            "180/180 [==============================] - 0s 824us/step - loss: 2.2380e-04 - acc: 0.8577\n",
            "Epoch 63/100\n",
            "180/180 [==============================] - 0s 843us/step - loss: 2.2379e-04 - acc: 0.8577\n",
            "Epoch 64/100\n",
            "180/180 [==============================] - 0s 824us/step - loss: 2.2372e-04 - acc: 0.8577\n",
            "Epoch 65/100\n",
            "180/180 [==============================] - 0s 825us/step - loss: 2.2364e-04 - acc: 0.8577\n",
            "Epoch 66/100\n",
            "180/180 [==============================] - 0s 812us/step - loss: 2.2363e-04 - acc: 0.8577\n",
            "Epoch 67/100\n",
            "180/180 [==============================] - 0s 828us/step - loss: 2.2359e-04 - acc: 0.8577\n",
            "Epoch 68/100\n",
            "180/180 [==============================] - 0s 800us/step - loss: 2.2355e-04 - acc: 0.8577\n",
            "Epoch 69/100\n",
            "180/180 [==============================] - 0s 832us/step - loss: 2.2355e-04 - acc: 0.8577\n",
            "Epoch 70/100\n",
            "180/180 [==============================] - 0s 875us/step - loss: 2.2348e-04 - acc: 0.8577\n",
            "Epoch 71/100\n",
            "180/180 [==============================] - 0s 849us/step - loss: 2.2345e-04 - acc: 0.8577\n",
            "Epoch 72/100\n",
            "180/180 [==============================] - 0s 853us/step - loss: 2.2341e-04 - acc: 0.8577\n",
            "Epoch 73/100\n",
            "180/180 [==============================] - 0s 795us/step - loss: 2.2338e-04 - acc: 0.8577\n",
            "Epoch 74/100\n",
            "180/180 [==============================] - 0s 814us/step - loss: 2.2335e-04 - acc: 0.8577\n",
            "Epoch 75/100\n",
            "180/180 [==============================] - 0s 826us/step - loss: 2.2333e-04 - acc: 0.8577\n",
            "Epoch 76/100\n",
            "180/180 [==============================] - 0s 859us/step - loss: 2.2331e-04 - acc: 0.8577\n",
            "Epoch 77/100\n",
            "180/180 [==============================] - 0s 855us/step - loss: 2.2328e-04 - acc: 0.8577\n",
            "Epoch 78/100\n",
            "180/180 [==============================] - 0s 874us/step - loss: 2.2325e-04 - acc: 0.8577\n",
            "Epoch 79/100\n",
            "180/180 [==============================] - 0s 820us/step - loss: 2.2327e-04 - acc: 0.8577\n",
            "Epoch 80/100\n",
            "180/180 [==============================] - 0s 848us/step - loss: 2.2325e-04 - acc: 0.8577\n",
            "Epoch 81/100\n",
            "180/180 [==============================] - 0s 827us/step - loss: 2.2321e-04 - acc: 0.8577\n",
            "Epoch 82/100\n",
            "180/180 [==============================] - 0s 834us/step - loss: 2.2312e-04 - acc: 0.8577\n",
            "Epoch 83/100\n",
            "180/180 [==============================] - 0s 802us/step - loss: 2.2311e-04 - acc: 0.8577\n",
            "Epoch 84/100\n",
            "180/180 [==============================] - 0s 882us/step - loss: 2.2310e-04 - acc: 0.8577\n",
            "Epoch 85/100\n",
            "180/180 [==============================] - 0s 817us/step - loss: 2.2306e-04 - acc: 0.8577\n",
            "Epoch 86/100\n",
            "180/180 [==============================] - 0s 817us/step - loss: 2.2302e-04 - acc: 0.8577\n",
            "Epoch 87/100\n",
            "180/180 [==============================] - 0s 806us/step - loss: 2.2298e-04 - acc: 0.8577\n",
            "Epoch 88/100\n",
            "180/180 [==============================] - 0s 815us/step - loss: 2.2296e-04 - acc: 0.8577\n",
            "Epoch 89/100\n",
            "180/180 [==============================] - 0s 801us/step - loss: 2.2294e-04 - acc: 0.8577\n",
            "Epoch 90/100\n",
            "180/180 [==============================] - 0s 871us/step - loss: 2.2293e-04 - acc: 0.8577\n",
            "Epoch 91/100\n",
            "180/180 [==============================] - 0s 823us/step - loss: 2.2294e-04 - acc: 0.8577\n",
            "Epoch 92/100\n",
            "180/180 [==============================] - 0s 813us/step - loss: 2.2284e-04 - acc: 0.8577\n",
            "Epoch 93/100\n",
            "180/180 [==============================] - 0s 788us/step - loss: 2.2282e-04 - acc: 0.8577\n",
            "Epoch 94/100\n",
            "180/180 [==============================] - 0s 812us/step - loss: 2.2282e-04 - acc: 0.8577\n",
            "Epoch 95/100\n",
            "180/180 [==============================] - 0s 820us/step - loss: 2.2282e-04 - acc: 0.8577\n",
            "Epoch 96/100\n",
            "180/180 [==============================] - 0s 805us/step - loss: 2.2284e-04 - acc: 0.8577\n",
            "Epoch 97/100\n",
            "180/180 [==============================] - 0s 854us/step - loss: 2.2276e-04 - acc: 0.8577\n",
            "Epoch 98/100\n",
            "180/180 [==============================] - 0s 832us/step - loss: 2.2272e-04 - acc: 0.8577\n",
            "Epoch 99/100\n",
            "180/180 [==============================] - 0s 804us/step - loss: 2.2268e-04 - acc: 0.8577\n",
            "Epoch 100/100\n",
            "180/180 [==============================] - 0s 776us/step - loss: 2.2266e-04 - acc: 0.8577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f273955ae48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXYJKsJ5ckLT",
        "colab_type": "text"
      },
      "source": [
        "##Teste da rede\n",
        "\n",
        "Aqui a rede é testada, sem produzir de fato saidas legíveis, mas apenas mostrando uma acurácia da rede em relação ao vetor bert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QS0KNZscwX3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "873d6c9f-d6af-4848-c433-05a951b5a72a"
      },
      "source": [
        "print(xt.shape)\n",
        "\n",
        "aaaaa = xr[0].reshape(1, 56, 768, 1)\n",
        "bbbbb = yr[0].reshape(56,768)\n",
        "\n",
        "ttttt = model.predict(aaaaa)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(180, 56, 768, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fii8I_hiMzqS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "a9fbf185-f031-436d-a6ec-51b8c3e371c5"
      },
      "source": [
        "ttttt = ttttt.reshape(56,768)\n",
        "print(ttttt.shape)\n",
        "\n",
        "asdf = vec2head(ttttt,dicio,0.5)\n",
        "print(asdf)\n",
        "\n",
        "fdsa = vec2head(bbbbb,dicio,0.5)\n",
        "print(fdsa)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(56, 768)\n",
            "[[('201', 0.6480411291122437)], [(\"##'\", 0.64035964012146)], [(\"##'\", 0.6887937784194946)], [('apr', 0.5055166482925415)], [(\"##'\", 0.6681265830993652)], [('201', 0.593322217464447)], [('`', 0.6478577852249146)], [('2013', 0.5896731019020081)], [('`', 0.6571579575538635)], [('##t', 0.5985698699951172)], [(\"##'\", 0.6559717059135437)], [('announces', 0.5409097671508789)], [('announced', 0.6185086965560913)], [('today', 0.6996063590049744)], [('that', 0.7532937526702881)], [('it', 0.6359387040138245)], [('has', 0.6615070104598999)], [('completed', 0.593213677406311)], [('announced', 0.541196346282959)], [('previously', 0.6193296909332275)], [('announced', 0.6645625829696655)], [('acquisition', 0.6641820669174194)], [('##o', 0.5461447834968567)], [('bel', 0.6513062715530396)], [('##o', 0.6455355882644653)], [('##5', 0.5406655073165894)], [('per', 0.574411928653717)], [('##5', 0.610558807849884)], [('per', 0.6707724332809448)], [('share', 0.6802378296852112)], [('cash', 0.5711758136749268)], [('cash', 0.7157635688781738)], [('addition', 0.5478994846343994)], [('in', 0.6123237013816833)], [('addition', 0.6308324933052063)], [('to', 0.603583812713623)], [('the', 0.6093595027923584)], [('assumption', 0.6426522731781006)], [('million', 0.583282470703125)], [('##5', 0.6549897789955139)], [('million', 0.6953192949295044)], [('debt', 0.5459762215614319)], [('outstanding', 0.6889035105705261)], [('debt', 0.7212722301483154)], [('for', 0.6437586545944214)], [('total', 0.5115770101547241)], [('total', 0.6686638593673706)], [('transaction', 0.6320469379425049)], [('value', 0.6926178932189941)], [('billion', 0.5904902219772339)], [('billion', 0.6477465629577637)], [('billion', 0.8120405673980713)]]\n",
            "[[('##t', 1.0)], [('completed', 0.5599807500839233)], [('##s', 0.9999998807907104)], [('acquisition', 1.0)], [('of', 1.0)], [('bel', 1.0)], [('##o', 1.0)]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZGIJBRi5J7m",
        "colab_type": "text"
      },
      "source": [
        "## Teste de predição\n",
        "\n",
        "Aqui a rede é rodada como teste. A metrica mais usada para esse teste é o ROUGE-1, que conta quantas palavras da saída do sistema está dentro do texto de referência, depois divide pelo número de palavras no texto de referência."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFw4uRsm5Kuu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "de48a4cc-52aa-4077-d7a4-b6a0ddab6371"
      },
      "source": [
        "\n",
        "\n",
        "#rouge-1\n",
        "def rouge(src, ref):\n",
        "  acertos = 0\n",
        "  for i in src:\n",
        "    if i in ref:\n",
        "      acertos += 1\n",
        "  return acertos/float(len(ref))\n",
        "\n",
        "sistem = ['five', 'plane']\n",
        "refere = ['five', 'in', 'plane']\n",
        "\n",
        "si = []\n",
        "re = []\n",
        "\n",
        "for i in asdf:\n",
        "  si.append(i[0])\n",
        "\n",
        "for i in fdsa:\n",
        "  re.append(i[0])\n",
        "\n",
        "print(rouge(sistem, refere))\n",
        "print(rouge(si,re))\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6666666666666666\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK4CtNKfAFwv",
        "colab_type": "text"
      },
      "source": [
        "## Referencias\n",
        "\n"
      ]
    }
  ]
}