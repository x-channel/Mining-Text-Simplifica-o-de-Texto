{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projeto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/projeto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP583IXGNeHC",
        "colab_type": "text"
      },
      "source": [
        "# Projeto de Simplificação de Texto\n",
        "\n",
        "Nas primeiras abordagens, houve uma tentativa de produzir uma rede neural que tivesse como entrada o *token frequence* e saida outro *token frequence*.\n",
        " Apesar disso poder ser considerado um resultado válido para a tentativa de melhorar a classificação de texto, não podemos considerar uma frase como um conjunto de palavras com a propriedade da comutatividade.\n",
        "  A ordem das palavras acaba importando muito para os humanos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptDnWb-pbJUb",
        "colab_type": "text"
      },
      "source": [
        "## importando bugingangas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh5apxspbJ0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import random\n",
        "#import pandas as pd\n",
        "from urllib import request as req\n",
        "from gensim.models import keyedvectors as kv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQFno4IJOzW3",
        "colab_type": "text"
      },
      "source": [
        "## Paper With Code\n",
        "\n",
        "~~Após um gole de sorte, eu acabo por encontrar essa maravilha~~ chamado de [Paper With Code](https://paperswithcode.com/sota/document-summarization-on-cnn-daily-mail), é um ~~bom~~ compilado de resultados cientificos sobre determinados problemas da computação. O primeiro artigo do link de cima, mostra uma solução com ROGUE-1 de 43.83 para o problema de sumarização de documentos como o *state of the art*.\n",
        "\n",
        "![paper with code](https://github.com/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/imagens/paperwithcode.png?raw=true)\n",
        "<center>paper with code</center>\n",
        "\n",
        "No artigo *Text Summarization with Pretrained Encoders* (LIU e LAPATA, 2019), os autores representam o texto como *Bert*, que é gerado por uma rede neural e é um caso especial do *word2vec*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzGa9dI5viX",
        "colab_type": "text"
      },
      "source": [
        "## Representação do Bert word2vec\n",
        "\n",
        "*Kyubyong* criou um preset do Bert, um dicionário onde cada palavra é representada por um vetor de 768 valores decimais.\n",
        "Visto algumas limitações das plataformas *Google Colab* e do *Github*, serão usadas apenas 300 instâncias do CNN, que gerou um dicionário reduzido de apenas 25mb.\n",
        "\n",
        "![Bert is Evil](https://github.com/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/imagens/audio-banner.jpg?raw=true)\n",
        "<center>Bert is Evil</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iap7h5R65zWn",
        "colab_type": "text"
      },
      "source": [
        "## Base de dados da Cable News Network\n",
        "\n",
        "A base de dados pode ser encontrado [nesse github da google](https://github.com/google-research-datasets/sentence-compression), no formato *Json*. Essa base de dados é formada com notícias da CNN com a primeira linha da noticia, com o título, com todos os bigramas possíveis e com informações do TAG. Porém no trabalho apenas será usado a primeira linha como entrada e o título como saída.\n",
        "\n",
        "![Json](https://github.com/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/imagens/Jasonf.jpg?raw=true)\n",
        "<center>Json</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Nkg503WqUi",
        "colab_type": "text"
      },
      "source": [
        "## Mesclar o bert com o json.\n",
        "\n",
        "Basicamente esse script carrega o vocabulário do dicionário bert, onde a partir da segunda linha, cada linha é uma palavra, seguida de sua representação vetorial com 768 valores.\n",
        "Eventualmente aparecem algumas palavras estranhas como \"##,\".\n",
        "Essa palavra significa somente que a palavra anterior termina com uma virgula.\n",
        "\n",
        "Depois de carregar o vocabulário, ele começa a ler as noticias do json.\n",
        "Ele segmenta uma noticia, a transformando em um dicionário do python, procura os textos alvos\n",
        "### jfk['graph']['sentence'] e jfk['headline'].\n",
        "\n",
        "Com os textos em mãos, ele cata as palavras de cada texto, criando uma matriz de tamanho 768*2 pela quantidade de linhas do subtitulo da noticia.\n",
        "\n",
        "Observar que para esse código rodar ele deve estar na seguinte pasta ~~por isso está comentado~~. Com o json nomeado *in.json*, com o bert nomeado *dicionariolongo.vec* e uma pasta vazia chamada de *instancias*. O script vai gerar um *dicionario.csv* e varias noticias dentro da pasta instancias. Infelizmente algumas noticias tem caracters que não permitem seu uso como nome do arquivo, por isso todas as noticias foram nomeadas de *arquivo_n.csv* ~~Outro motivo para você guardar seus dados em um único arquivo json~~.\n",
        "\n",
        ">O diretório do arquivo.\n",
        ">>in.json \\n\n",
        "dicionariolongo.vec \\n\n",
        "preprobert.py\n",
        ">>> instancias\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Uesl-rWdP1",
        "colab_type": "code",
        "outputId": "7fdf54b0-2e68-4e3b-b415-0a7b42ccd900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "#texto para matriz Bert\n",
        "\n",
        "#!/usr/bin/python\n",
        "# -*- coding: UTF-8 -*-\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import fileinput\n",
        "\n",
        "import codecs\n",
        "\n",
        "import os\n",
        "\n",
        "dici = {}\n",
        "fins = {}\n",
        "dico = {} #dicionario reduzido\n",
        "\n",
        "\n",
        "#carrega o dicionario bert\n",
        "with open(\"dicionariolongo.vec\", \"r\", encoding=\"utf-8\") as d:\n",
        "    for i in d:\n",
        "        if len(i) > 200: #isso aqui eh para tirar a primeira linha, altura x largura\n",
        "            try:\n",
        "                a = i.split()\n",
        "                if '##' in a[0]:\n",
        "                    fins[a[0][2:].lower()] = []\n",
        "                    for j in a[1:]:\n",
        "                        fins[a[0][2:].lower()].append(float(j))\n",
        "                elif (not a[0].lower() in dici) or a[0].islower():\n",
        "                    dici[a[0].lower()] = []\n",
        "                    for j in a[1:]:\n",
        "                        dici[a[0].lower()].append(float(j))\n",
        "            except ValueError:\n",
        "                print(\"problema com uma palavra\")\n",
        "\n",
        "\n",
        "\n",
        "with open(\"in.json\", 'rb') as f:\n",
        "    data = f.read()\n",
        "\n",
        "data = str(data)[2:-1]\n",
        "data = data.replace('\\\\n}', '\\\\n}-----')\n",
        "data = data.split('-----')\n",
        "\n",
        "blanckl = []\n",
        "for i in range(768):\n",
        "    blanckl.append('')\n",
        "\n",
        "instancias = 0\n",
        "for p in data[0:-1]:\n",
        "    entrada = []\n",
        "    saida = []\n",
        "    try:\n",
        "        jfk = json.loads(codecs.decode(p, 'unicode_escape'))\n",
        "        arquivo = jfk['graph']['sentence'].lower().replace(\"/\",\"\").replace(\"\\\\\",\"\")\n",
        "        lg = jfk['graph']['sentence'].lower().split()\n",
        "        st = jfk['headline'].lower().split()\n",
        "\n",
        "        for i in lg: #isso deveria ser uma funcao, addlist() #semTempo\n",
        "            if i in dico: #dicionario reduzido\n",
        "                entrada.append(dico[i])\n",
        "            elif i in dici:\n",
        "                dico[i] = dici[i]\n",
        "                entrada.append(dico[i])\n",
        "            else:\n",
        "                for j, jj in fins.items():\n",
        "                    if j in i[-len(j):]:\n",
        "                        i = i[:-len(j)]\n",
        "                        if i in dico: #assim ela poderia ser chamada aqui\n",
        "                            entrada.append(dico[i])\n",
        "                        elif i in dici:\n",
        "                            dico[i] = dici[i]\n",
        "                            entrada.append(dico[i])\n",
        "                        if j in dico:\n",
        "                            entrada.append(dico[j])\n",
        "                        else:\n",
        "                            dico[j] = fins[j]\n",
        "                            entrada.append(dico[j])\n",
        "                        break\n",
        "        for i in st: #isso deveria ser uma funcao, addlist() #semTempo\n",
        "            if i in dico: #dicionario reduzido\n",
        "                saida.append(dico[i])\n",
        "            elif i in dici:\n",
        "                dico[i] = dici[i]\n",
        "                saida.append(dico[i])\n",
        "            else:\n",
        "                for j, jj in fins.items():\n",
        "                    if j in i[-len(j):]:\n",
        "                        i = i[:-len(j)]\n",
        "                        if i in dico: #assim ela poderia ser chamada aqui\n",
        "                            saida.append(dico[i])\n",
        "                        elif i in dici:\n",
        "                            dico[i] = dici[i]\n",
        "                            saida.append(dico[i])\n",
        "                        if j in dico:\n",
        "                            saida.append(dico[j])\n",
        "                        else:\n",
        "                            dico[j] = fins[j]\n",
        "                            saida.append(dico[j])\n",
        "                        break\n",
        "    except ValueError:\n",
        "        print(\"Houve um Erro\")\n",
        "\n",
        "        \n",
        "    sv = os.getcwd() + '\\\\instancias\\\\arquivo_%i'%instancias + '.csv'\n",
        "    instancias += 1\n",
        "    try:\n",
        "        with open(sv, 'w', newline = '') as file:\n",
        "            writer = csv.writer(file)\n",
        "            tam = 0\n",
        "            while tam < len(entrada):\n",
        "                tm = []\n",
        "                #print(type(tm))\n",
        "                tm.extend(entrada[tam])\n",
        "                if tam < len(saida):\n",
        "                    tm.extend(saida[tam])\n",
        "                else:\n",
        "                    tm.extend(blanckl)\n",
        "                tam += 1\n",
        "                writer.writerow(tm)\n",
        "    except ValueError:\n",
        "        print(\"erro no json\")\n",
        "\n",
        "with open(\"dicionario.csv\", \"w\", newline = '') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for i, ii in dico.items():\n",
        "\ttm = []\n",
        "\ttm.append(i)\n",
        "\ttm.extend(ii)\n",
        "        writer.writerow(tm)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#texto para matriz Bert\\n\\n#!/usr/bin/python\\n# -*- coding: UTF-8 -*-\\n\\nimport pandas as pd\\nimport csv\\n\\nimport nltk\\nimport numpy as np\\n\\nimport json\\nimport fileinput\\n\\nimport codecs\\n\\nimport os\\n\\ndici = {}\\nfins = {}\\ndico = {} #dicionario reduzido\\n\\n\\n#carrega o dicionario bert\\nwith open(\"dicionariolongo.vec\", \"r\", encoding=\"utf-8\") as d:\\n    for i in d:\\n        if len(i) > 200: #isso aqui eh para tirar a primeira linha, altura x largura\\n            try:\\n                a = i.split()\\n                if \\'##\\' in a[0]:\\n                    fins[a[0][2:].lower()] = []\\n                    for j in a[1:]:\\n                        fins[a[0][2:].lower()].append(float(j))\\n                elif (not a[0].lower() in dici) or a[0].islower():\\n                    dici[a[0].lower()] = []\\n                    for j in a[1:]:\\n                        dici[a[0].lower()].append(float(j))\\n            except ValueError:\\n                print(\"problema com uma palavra\")\\n\\n\\n\\nwith open(\"in.json\", \\'rb\\') as f:\\n    data = f.read()\\n\\ndata = str(data)[2:-1]\\ndata = data.replace(\\'\\\\n}\\', \\'\\\\n}-----\\')\\ndata = data.split(\\'-----\\')\\n\\nblanckl = []\\nfor i in range(768):\\n    blanckl.append(\\'\\')\\n\\ninstancias = 0\\nfor p in data[0:-1]:\\n    entrada = []\\n    saida = []\\n    try:\\n        jfk = json.loads(codecs.decode(p, \\'unicode_escape\\'))\\n        arquivo = jfk[\\'graph\\'][\\'sentence\\'].lower().replace(\"/\",\"\").replace(\"\\\\\",\"\")\\n        lg = jfk[\\'graph\\'][\\'sentence\\'].lower().split()\\n        st = jfk[\\'headline\\'].lower().split()\\n\\n        for i in lg: #isso deveria ser uma funcao, addlist() #semTempo\\n            if i in dico: #dicionario reduzido\\n                entrada.append(dico[i])\\n            elif i in dici:\\n                dico[i] = dici[i]\\n                entrada.append(dico[i])\\n            else:\\n                for j, jj in fins.items():\\n                    if j in i[-len(j):]:\\n                        i = i[:-len(j)]\\n                        if i in dico: #assim ela poderia ser chamada aqui\\n                            entrada.append(dico[i])\\n                        elif i in dici:\\n                            dico[i] = dici[i]\\n                            entrada.append(dico[i])\\n                        if j in dico:\\n                            entrada.append(dico[j])\\n                        else:\\n                            dico[j] = fins[j]\\n                            entrada.append(dico[j])\\n                        break\\n        for i in st: #isso deveria ser uma funcao, addlist() #semTempo\\n            if i in dico: #dicionario reduzido\\n                saida.append(dico[i])\\n            elif i in dici:\\n                dico[i] = dici[i]\\n                saida.append(dico[i])\\n            else:\\n                for j, jj in fins.items():\\n                    if j in i[-len(j):]:\\n                        i = i[:-len(j)]\\n                        if i in dico: #assim ela poderia ser chamada aqui\\n                            saida.append(dico[i])\\n                        elif i in dici:\\n                            dico[i] = dici[i]\\n                            saida.append(dico[i])\\n                        if j in dico:\\n                            saida.append(dico[j])\\n                        else:\\n                            dico[j] = fins[j]\\n                            saida.append(dico[j])\\n                        break\\n    except ValueError:\\n        print(\"Houve um Erro\")\\n\\n        \\n    sv = os.getcwd() + \\'\\\\instancias\\\\arquivo_%i\\'%instancias + \\'.csv\\'\\n    instancias += 1\\n    try:\\n        with open(sv, \\'w\\', newline = \\'\\') as file:\\n            writer = csv.writer(file)\\n            tam = 0\\n            while tam < len(entrada):\\n                tm = []\\n                #print(type(tm))\\n                tm.extend(entrada[tam])\\n                if tam < len(saida):\\n                    tm.extend(saida[tam])\\n                else:\\n                    tm.extend(blanckl)\\n                tam += 1\\n                writer.writerow(tm)\\n    except ValueError:\\n        print(\"erro no json\")\\n\\nwith open(\"dicionario.csv\", \"w\", newline = \\'\\') as file:\\n    writer = csv.writer(file)\\n    for i, ii in dico.items():\\n\\ttm = []\\n\\ttm.append(i)\\n\\ttm.extend(ii)\\n        writer.writerow(tm)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKjJH3pbaixJ",
        "colab_type": "text"
      },
      "source": [
        "## Abrindo uma noticia (bert) e convertendo para texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6WEsqOPajg1",
        "colab_type": "code",
        "outputId": "eb50f475-cbfb-4fb1-85fb-ec8770c0fb6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "# Abrindo o dicionario criado com o outro script\n",
        "dicurl = 'https://raw.githubusercontent.com/x-channel/Mining-Text-Simplifica-o-de-Texto/master/dataset/dicionario.csv'\n",
        "#dicloader = req.urlopen(dicurl)\n",
        "vetores = []\n",
        "palavras = []\n",
        "\n",
        "with req.urlopen(dicurl) as f:\n",
        "  meucsv = f.read().decode('charmap')\n",
        "  meucsv = meucsv.split('\\n')[:-1]\n",
        "  for i in meucsv:\n",
        "    j = i.replace('\\r','').split(',')\n",
        "    palavras.append(j[0])\n",
        "    vetores.append(j[1:])\n",
        "\n",
        "# colocando o dicionario no gensim.\n",
        "dicio = kv.Word2VecKeyedVectors(768)\n",
        "dicio.add(palavras, np.array(vetores).astype(float))\n",
        "\n",
        "#busca por um vetor parecido\n",
        "five = dicio.get_vector('five')\n",
        "friends = dicio.get_vector('friends')\n",
        "\n",
        "print(dicio.similar_by_vector(five, 1), dicio.similar_by_vector(friends, 1))\n",
        "\n",
        "#abrindo a noticia\n",
        "#nn = input(\"digite um numero entre 0 e 199: \")\n",
        "nn = 1\n",
        "noticia = 'https://raw.githubusercontent.com/x-channel/Mining-Text-Simplifica-o-de-Texto/master/dataset/noticias/arquivo_%i.csv'%nn\n",
        "\n",
        "pastaNoticias = 'https://raw.githubusercontent.com/x-channel/Mining-Text-Simplifica-o-de-Texto/master/dataset/noticias/%s_%i.csv'\n",
        "\n",
        "def abrirNoticia(urlnoticia):\n",
        "  sentence = []\n",
        "  head = []\n",
        "  with req.urlopen(urlnoticia) as f:\n",
        "    meucsv = f.read().decode('charmap')\n",
        "    meucsv = meucsv.split('\\n')[:-1]\n",
        "    for i in meucsv:\n",
        "      j = i.replace('\\r',',').replace(',,', ',0.0,').replace(',,', ',0.0,')[:-1]\n",
        "      j = j.split(',')\n",
        "      sentence.append(j[:768])\n",
        "      head.append(j[768:])\n",
        "  sentence = np.array(sentence).astype(float)\n",
        "  head = np.array(head).astype(float)\n",
        "  return sentence, head\n",
        "\n",
        "def abrirNoticias(urlfolder, total, nome = 'arquivo'):\n",
        "  primeiro = []\n",
        "  cabecalh = []\n",
        "  for i in range(total):\n",
        "    par = abrirNoticia(urlfolder%(nome,i))\n",
        "    primeiro.append(par[0])\n",
        "    cabecalh.append(par[1])\n",
        "  return np.array(primeiro),np.array(cabecalh)\n",
        "\n",
        "\n",
        "def vec2head(matriz, model, li):\n",
        "  head = []\n",
        "  for i in matriz:\n",
        "    j = model.similar_by_vector(i,1)\n",
        "    if j[0][1] > li:\n",
        "      head.append(j)\n",
        "  return head\n",
        "\n",
        "s, h = abrirNoticia(noticia)\n",
        "\n",
        "titulo = vec2head(h, dicio, 0.5)\n",
        "print (titulo)\n",
        "\n",
        "noticias,cabecalhos = abrirNoticias(pastaNoticias, 200)\n",
        "print(vec2head(noticias[1], dicio, 0.5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('five', 1.0)] [('friends', 1.0)]\n",
            "[[('several', 1.0)], [('school', 1.0)], [('districts', 1.0)], [('hold', 1.0)], [('classes', 1.0)], [('on', 1.0)], [('president', 0.6351367831230164)], [(\"##'\", 0.9999998807907104)], [('day', 1.0000001192092896)], [('to', 1.0)], [('make', 1.0)], [('up', 1.0000001192092896)], [('for', 1.0)], [('days', 1.0)], [('missed', 1.0)]]\n",
            "[[('several', 1.0)], [('school', 1.0)], [('districts', 1.0)], [('in', 1.0)], [('hampton', 1.0)], [('roads', 1.0)], [('are', 0.9999999403953552)], [('holding', 1.0)], [('classes', 1.0)], [('this', 1.0000001192092896)], [('president', 0.6351367831230164)], [(\"##'\", 0.9999998807907104)], [('day', 1.0000001192092896)], [('to', 1.0)], [('make', 1.0)], [('up', 1.0000001192092896)], [('for', 1.0)], [('days', 1.0)], [('missed', 1.0)], [('because', 1.0)], [('of', 1.0)], [('the', 1.0)], [(\"##'\", 0.696808934211731)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNJpRDlg4kQW",
        "colab_type": "text"
      },
      "source": [
        "## Dividindo a base de dados\n",
        "\n",
        "Aqui a base de dados é dividida em: Treinamento, validação e teste.\n",
        "\n",
        "OBS: não encontrei isso implementado nem no scikit learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AorJ_zzpsZIO",
        "colab_type": "code",
        "outputId": "b0840278-ba91-4884-8395-757bee6684ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#pading, para tudo ter o mesmo tamanho\n",
        "\n",
        "maxlen = 0\n",
        "\n",
        "for i in noticias:\n",
        "  if i.shape[0] > maxlen:\n",
        "    maxlen = i.shape[0]\n",
        "\n",
        "for i in range(len(noticias)):\n",
        "  pad = np.zeros((maxlen, 768))\n",
        "  pad[:noticias[i].shape[0],:] = noticias[i]\n",
        "  noticias[i] = pad\n",
        "  pad = np.zeros((maxlen, 768))\n",
        "  pad[:cabecalhos[i].shape[0],:] = cabecalhos[i]\n",
        "  cabecalhos[i] = pad\n",
        "\n",
        "print(maxlen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9psE79f5CmX",
        "colab_type": "code",
        "outputId": "395f7f69-7286-49d6-ebcd-a9445219a1ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "treinamento = []\n",
        "#validacao = []\n",
        "teste = []\n",
        "\n",
        "xt = []\n",
        "yt = []\n",
        "\n",
        "xr = []\n",
        "yr = []\n",
        "\n",
        "atreino = 0.9\n",
        "#avalidacao = 0.3\n",
        "\n",
        "d = []\n",
        "for i in range(len(noticias)):\n",
        "  d.append(i)\n",
        "\n",
        "random.shuffle(d)\n",
        "\n",
        "# deixar essa linha para validar os parametros dos primeiros testes\n",
        "d = [117, 135, 181, 2, 129, 167, 65, 183, 107, 104, 158, 111, 69, 194, 8, 101, 21, 35, 31, 188, 106, 196, 148, 198, 67, 60, 102, 82, 16, 88, 119, 61, 11, 115, 113, 56, 169, 98, 64, 40, 49, 162, 36, 127, 157, 66, 164, 180, 41, 138, 62, 34, 72, 178, 27, 189, 121, 154, 96, 14, 133, 145, 97, 43, 199, 51, 25, 163, 155, 47, 70, 150, 12, 30, 123, 195, 32, 55, 18, 176, 171, 68, 175, 120, 110, 59, 141, 6, 23, 44, 103, 151, 125, 130, 79, 73, 173, 1, 58, 165, 118, 46, 39, 191, 10, 74, 166, 24, 147, 131, 190, 20, 156, 26, 22, 187, 182, 75, 63, 52, 9, 132, 87, 5, 144, 192, 42, 142, 90, 85, 143, 13, 153, 174, 122, 139, 184, 128, 19, 50, 161, 172, 168, 83, 48, 71, 185, 53, 126, 4, 29, 86, 15, 7, 92, 45, 197, 76, 134, 37, 54, 152, 57, 84, 112, 3, 28, 93, 0, 109, 136, 177, 77, 170, 100, 146, 137, 179, 80, 33, 17, 124, 89, 193, 38, 160, 78, 95, 140, 114, 159, 81, 186, 99, 108, 105, 116, 94, 149, 91]\n",
        "\n",
        "for i in range(len(noticias)):\n",
        "  if i < atreino*len(noticias):\n",
        "    xt.append(noticias[d[i]])\n",
        "    yt.append(cabecalhos[d[i]])\n",
        "  else:\n",
        "    xr.append(noticias[d[i]])\n",
        "    yr.append(cabecalhos[d[i]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "xt = np.array(xt)\n",
        "yt = np.array(yt)\n",
        "\n",
        "xr = np.array(xr)\n",
        "yr = np.array(yr)\n",
        "\n",
        "xt = xt.reshape(xt.shape[0], maxlen, 768, 1)\n",
        "yt = yt.reshape(yt.shape[0], maxlen, 768, 1)\n",
        "\n",
        "xr = xr.reshape(xr.shape[0], maxlen, 768, 1)\n",
        "yr = yr.reshape(yr.shape[0], maxlen, 768, 1)\n",
        "\n",
        "print(len(xt), len(yt))\n",
        "print(len(xr), len(xr[1]), len(xr[1][1]))\n",
        "\n",
        "'''\n",
        "for i in range(len(noticias)):\n",
        "  if i < atreino*len(noticias):\n",
        "    treinamento.append(noticias[d[i]])\n",
        "  elif i < (atreino+avaliacao)*len(noticias):\n",
        "    validacao.append(noticias[d[i]])\n",
        "  else:\n",
        "    teste.append(noticias[d[i]])'''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "180 180\n",
            "20 56 768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor i in range(len(noticias)):\\n  if i < atreino*len(noticias):\\n    treinamento.append(noticias[d[i]])\\n  elif i < (atreino+avaliacao)*len(noticias):\\n    validacao.append(noticias[d[i]])\\n  else:\\n    teste.append(noticias[d[i]])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7aMectc4YIn",
        "colab_type": "text"
      },
      "source": [
        "## Construindo a Rede Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx1l-6QCQCSY",
        "colab_type": "code",
        "outputId": "75fed62e-1a32-4abb-d217-36915d4ac4e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "#Importando o keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Deconvolution2D, Reshape\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thAFO7kN4eFU",
        "colab_type": "code",
        "outputId": "aed99c02-c992-4a39-ccc9-b18b0d0c6f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "model = Sequential()\n",
        "#model.add(Conv2D(2, kernel_size=(3,3), activation = \"tanh\", input_shape=(maxlen,768,1)))\n",
        "con = Conv2D(2, kernel_size=(3,3), activation = \"tanh\", input_shape=(maxlen,768,1),padding=\"same\")\n",
        "model.add(con)\n",
        "print(con.input_shape)\n",
        "print(con.output_shape)\n",
        "model.add(Conv2D(4, (2,2), activation = \"tanh\",padding=\"same\"))\n",
        "print(model.output_shape)\n",
        "#model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "#print(model.output_shape)\n",
        "\n",
        "print()\n",
        "model.add(Deconvolution2D(1, (2,2),padding=\"same\", dilation_rate =2))\n",
        "print(model.output_shape)\n",
        "\n",
        "#model.add(Reshape((maxlen,768,1)))\n",
        "#print(model.output_shape)\n",
        "\n",
        "#model.add(Conv2DTranspose(1,(2,2),output_shape=(768,), activation=\"tanh\"))\n",
        "#print(model.output_shape)\n",
        "#rs = Reshape((maxlen,768,1))\n",
        "#print(rs.output_shape)\n",
        "#print(rs.input_shape)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',loss='mse',metrics=[\"acc\"])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 56, 768, 1)\n",
            "(None, 56, 768, 2)\n",
            "(None, 56, 768, 4)\n",
            "\n",
            "(None, 56, 768, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnC1bMvV4eme",
        "colab_type": "text"
      },
      "source": [
        "## Treinando a rede"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyMsp1u95Jo2",
        "colab_type": "code",
        "outputId": "2ecf3a94-a328-4c49-fd60-f3910d3cfa1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(xt,yt,epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "180/180 [==============================] - 8s 42ms/step - loss: 3.6676e-04 - acc: 0.8577\n",
            "Epoch 2/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 3.3654e-04 - acc: 0.8577\n",
            "Epoch 3/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 3.1417e-04 - acc: 0.8577\n",
            "Epoch 4/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.9694e-04 - acc: 0.8577\n",
            "Epoch 5/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.8390e-04 - acc: 0.8577\n",
            "Epoch 6/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.7363e-04 - acc: 0.8577\n",
            "Epoch 7/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.6579e-04 - acc: 0.8577\n",
            "Epoch 8/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.5951e-04 - acc: 0.8577\n",
            "Epoch 9/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.5444e-04 - acc: 0.8577\n",
            "Epoch 10/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.5032e-04 - acc: 0.8577\n",
            "Epoch 11/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.4685e-04 - acc: 0.8577\n",
            "Epoch 12/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.4394e-04 - acc: 0.8577\n",
            "Epoch 13/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.4157e-04 - acc: 0.8577\n",
            "Epoch 14/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.3950e-04 - acc: 0.8577\n",
            "Epoch 15/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.3774e-04 - acc: 0.8577\n",
            "Epoch 16/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.3623e-04 - acc: 0.8577\n",
            "Epoch 17/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.3490e-04 - acc: 0.8577\n",
            "Epoch 18/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.3376e-04 - acc: 0.8577\n",
            "Epoch 19/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.3277e-04 - acc: 0.8577\n",
            "Epoch 20/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.3189e-04 - acc: 0.8577\n",
            "Epoch 21/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.3110e-04 - acc: 0.8577\n",
            "Epoch 22/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.3044e-04 - acc: 0.8577\n",
            "Epoch 23/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2983e-04 - acc: 0.8577\n",
            "Epoch 24/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2931e-04 - acc: 0.8577\n",
            "Epoch 25/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2884e-04 - acc: 0.8577\n",
            "Epoch 26/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2842e-04 - acc: 0.8577\n",
            "Epoch 27/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2806e-04 - acc: 0.8577\n",
            "Epoch 28/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2774e-04 - acc: 0.8577\n",
            "Epoch 29/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2746e-04 - acc: 0.8577\n",
            "Epoch 30/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2719e-04 - acc: 0.8577\n",
            "Epoch 31/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2693e-04 - acc: 0.8577\n",
            "Epoch 32/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2670e-04 - acc: 0.8577\n",
            "Epoch 33/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2650e-04 - acc: 0.8577\n",
            "Epoch 34/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2631e-04 - acc: 0.8577\n",
            "Epoch 35/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2620e-04 - acc: 0.8577\n",
            "Epoch 36/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2610e-04 - acc: 0.8577\n",
            "Epoch 37/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2594e-04 - acc: 0.8577\n",
            "Epoch 38/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2581e-04 - acc: 0.8577\n",
            "Epoch 39/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2569e-04 - acc: 0.8577\n",
            "Epoch 40/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2555e-04 - acc: 0.8577\n",
            "Epoch 41/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2540e-04 - acc: 0.8577\n",
            "Epoch 42/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2531e-04 - acc: 0.8577\n",
            "Epoch 43/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2521e-04 - acc: 0.8577\n",
            "Epoch 44/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2510e-04 - acc: 0.8577\n",
            "Epoch 45/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2499e-04 - acc: 0.8577\n",
            "Epoch 46/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2496e-04 - acc: 0.8577\n",
            "Epoch 47/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2484e-04 - acc: 0.8577\n",
            "Epoch 48/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2477e-04 - acc: 0.8577\n",
            "Epoch 49/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2469e-04 - acc: 0.8577\n",
            "Epoch 50/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2463e-04 - acc: 0.8577\n",
            "Epoch 51/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2455e-04 - acc: 0.8577\n",
            "Epoch 52/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2448e-04 - acc: 0.8577\n",
            "Epoch 53/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2446e-04 - acc: 0.8577\n",
            "Epoch 54/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2439e-04 - acc: 0.8577\n",
            "Epoch 55/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2429e-04 - acc: 0.8577\n",
            "Epoch 56/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2423e-04 - acc: 0.8577\n",
            "Epoch 57/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2420e-04 - acc: 0.8577\n",
            "Epoch 58/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2415e-04 - acc: 0.8577\n",
            "Epoch 59/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2408e-04 - acc: 0.8577\n",
            "Epoch 60/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2402e-04 - acc: 0.8577\n",
            "Epoch 61/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2394e-04 - acc: 0.8577\n",
            "Epoch 62/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2387e-04 - acc: 0.8577\n",
            "Epoch 63/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2384e-04 - acc: 0.8577\n",
            "Epoch 64/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2376e-04 - acc: 0.8577\n",
            "Epoch 65/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2373e-04 - acc: 0.8577\n",
            "Epoch 66/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2367e-04 - acc: 0.8577\n",
            "Epoch 67/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2365e-04 - acc: 0.8577\n",
            "Epoch 68/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2360e-04 - acc: 0.8577\n",
            "Epoch 69/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2355e-04 - acc: 0.8577\n",
            "Epoch 70/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2347e-04 - acc: 0.8577\n",
            "Epoch 71/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2344e-04 - acc: 0.8577\n",
            "Epoch 72/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2340e-04 - acc: 0.8577\n",
            "Epoch 73/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2337e-04 - acc: 0.8577\n",
            "Epoch 74/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2333e-04 - acc: 0.8577\n",
            "Epoch 75/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2328e-04 - acc: 0.8577\n",
            "Epoch 76/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2324e-04 - acc: 0.8577\n",
            "Epoch 77/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2319e-04 - acc: 0.8577\n",
            "Epoch 78/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2316e-04 - acc: 0.8577\n",
            "Epoch 79/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2316e-04 - acc: 0.8577\n",
            "Epoch 80/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2308e-04 - acc: 0.8577\n",
            "Epoch 81/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2303e-04 - acc: 0.8577\n",
            "Epoch 82/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2301e-04 - acc: 0.8577\n",
            "Epoch 83/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2297e-04 - acc: 0.8577\n",
            "Epoch 84/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2293e-04 - acc: 0.8577\n",
            "Epoch 85/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2291e-04 - acc: 0.8577\n",
            "Epoch 86/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2291e-04 - acc: 0.8577\n",
            "Epoch 87/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2285e-04 - acc: 0.8577\n",
            "Epoch 88/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2282e-04 - acc: 0.8577\n",
            "Epoch 89/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2278e-04 - acc: 0.8577\n",
            "Epoch 90/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2275e-04 - acc: 0.8577\n",
            "Epoch 91/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2278e-04 - acc: 0.8577\n",
            "Epoch 92/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2276e-04 - acc: 0.8577\n",
            "Epoch 93/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2276e-04 - acc: 0.8577\n",
            "Epoch 94/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2271e-04 - acc: 0.8577\n",
            "Epoch 95/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2269e-04 - acc: 0.8577\n",
            "Epoch 96/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2263e-04 - acc: 0.8577\n",
            "Epoch 97/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2261e-04 - acc: 0.8577\n",
            "Epoch 98/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2262e-04 - acc: 0.8577\n",
            "Epoch 99/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2267e-04 - acc: 0.8577\n",
            "Epoch 100/100\n",
            "180/180 [==============================] - 0s 1ms/step - loss: 2.2258e-04 - acc: 0.8577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f78fe888a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXYJKsJ5ckLT",
        "colab_type": "text"
      },
      "source": [
        "##Teste da rede\n",
        "\n",
        "Aqui a rede é testada, sem produzir de fato saidas legíveis, mas apenas mostrando uma acurácia da rede em relação ao vetor bert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QS0KNZscwX3",
        "colab_type": "code",
        "outputId": "bda7e823-69a7-4f48-9562-e005b7d933a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "print(xt.shape)\n",
        "\n",
        "aaaaa = xr[0].reshape(1, 56, 768, 1)\n",
        "bbbbb = yr[0].reshape(56,768)\n",
        "\n",
        "ttttt = model.predict(aaaaa)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-92d6505108b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maaaaa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbbbbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fii8I_hiMzqS",
        "colab_type": "code",
        "outputId": "abfe7c3e-10f9-4462-c17d-f7c6cd01197c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "ttttt = ttttt.reshape(56,768)\n",
        "print(ttttt.shape)\n",
        "\n",
        "asdf = vec2head(ttttt,dicio,0.5)\n",
        "print(asdf)\n",
        "\n",
        "fdsa = vec2head(bbbbb,dicio,0.5)\n",
        "print(fdsa)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(56, 768)\n",
            "[[('201', 0.6595737934112549)], [(\"##'\", 0.6446463465690613)], [(\"##'\", 0.6573339700698853)], [(\"##'\", 0.5432831048965454)], [(\"##'\", 0.6223398447036743)], [('201', 0.5796380639076233)], [('`', 0.6158039569854736)], [('2013', 0.5690800547599792)], [('`', 0.6030287146568298)], [('##t', 0.5703284740447998)], [(\"##'\", 0.6209338903427124)], [(\"##'\", 0.5253568887710571)], [('announced', 0.5673514604568481)], [('today', 0.6322273015975952)], [('today', 0.699204683303833)], [('that', 0.7536540031433105)], [('it', 0.6571808457374573)], [('has', 0.6315385103225708)], [('completed', 0.6037743091583252)], [('previously', 0.6270340085029602)], [('announced', 0.6219427585601807)], [('acquisition', 0.6369504332542419)], [('acquisition', 0.6638456583023071)], [('bel', 0.6234313249588013)], [('bel', 0.643802285194397)], [('##o', 0.6251477003097534)], [('##5', 0.5489922165870667)], [('##5', 0.5892611742019653)], [('per', 0.6156011819839478)], [('per', 0.640164315700531)], [('share', 0.658423900604248)], [('cash', 0.6922643184661865)], [('cash', 0.699795663356781)], [('addition', 0.5645601749420166)], [('addition', 0.5870875120162964)], [('to', 0.6178958415985107)], [('to', 0.6096583008766174)], [('the', 0.5603827834129333)], [('assumption', 0.6398985385894775)], [('##5', 0.6375483870506287)], [('million', 0.659980297088623)], [('million', 0.6873966455459595)], [('outstanding', 0.6335018873214722)], [('debt', 0.6725428104400635)], [('debt', 0.733618974685669)], [('for', 0.5038665533065796)], [('for', 0.6387195587158203)], [('total', 0.6516909599304199)], [('total', 0.6319130659103394)], [('value', 0.6599929928779602)], [('value', 0.6980164051055908)], [('##2', 0.5823213458061218)], [('billion', 0.7406005859375)], [('billion', 0.8020639419555664)], [(\"##'\", 0.5492129921913147)]]\n",
            "[[('##t', 1.0)], [('completed', 0.5599807500839233)], [('##s', 0.9999998807907104)], [('acquisition', 1.0)], [('of', 1.0)], [('bel', 1.0)], [('##o', 1.0)]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZGIJBRi5J7m",
        "colab_type": "text"
      },
      "source": [
        "## Teste de predição\n",
        "\n",
        "Aqui a rede é rodada como teste. A metrica mais usada para esse teste é o ROUGE-1, que conta quantas palavras da saída do sistema está dentro do texto de referência, depois divide pelo número de palavras no texto de referência."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFw4uRsm5Kuu",
        "colab_type": "code",
        "outputId": "e45b163d-bdd2-4578-a6ab-e2be246ddd6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "\n",
        "#rouge-1\n",
        "def rouge(src, ref):\n",
        "  acertos = 0\n",
        "  for i in src:\n",
        "    if i in ref:\n",
        "      acertos += 1\n",
        "  return acertos/float(len(ref))\n",
        "\n",
        "sistem = ['five', 'plane']\n",
        "refere = ['five', 'in', 'plane']\n",
        "\n",
        "si = []\n",
        "re = []\n",
        "\n",
        "for i in asdf:\n",
        "  si.append(i[0])\n",
        "\n",
        "for i in fdsa:\n",
        "  re.append(i[0])\n",
        "\n",
        "print(rouge(sistem, refere))\n",
        "print(rouge(si,re))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6666666666666666\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE9N8xIG5tBd",
        "colab_type": "text"
      },
      "source": [
        "# SHAME!\n",
        "\n",
        "Certo, o resultado de 85% de precisão da rede neural não produziu nenhuma sentença compatível com a sua referência. Vamos tentar outra coisa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0so6Iax2wMQ",
        "colab_type": "text"
      },
      "source": [
        "## Classificação\n",
        "\n",
        "Há uma base de dados que compreende recomendações dos jogos da steam. https://github.com/mulhod/steam_reviews\n",
        "\n",
        "\n",
        "Porém aqui será usado somente o word2vec, visto que a base de dados é outra e não é possível somente importar o dicionário do bert.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJri0htt2vT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To reimportando para nao precisar rodar tudo.\n",
        "import gensim\n",
        "import numpy as np\n",
        "import json\n",
        "import keras\n",
        "\n",
        "from urllib import request as req\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7htR7oL9tX3",
        "colab_type": "text"
      },
      "source": [
        "## importando a base de dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbidOvKs9si3",
        "colab_type": "code",
        "outputId": "0316df11-791a-48d3-f57a-9d7aab46ba33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        }
      },
      "source": [
        "#urlpastareviews = \"https://github.com/mulhod/steam_reviews/tree/master/data\"\n",
        "vetorarqui = [\"Arma_3.jsonlines\",\"Counter_Strike.jsonlines\",\"Counter_Strike_Global_Offensive.jsonlines\",\"Dota_2.jsonlines\",\"Football_Manager_2015.jsonlines\",\"Garrys_Mod.jsonlines\",\"Grand_Theft_Auto_V.jsonlines\",\"Sid_Meiers_Civilization_5.jsonlines\",\"Team_Fortress_2.jsonlines\",\"The_Elder_Scrolls_V.jsonlines\",\"Warframe.jsonlines\"]\n",
        "cadaReview = \"https://raw.githubusercontent.com/mulhod/steam_reviews/master/data/%s\"\n",
        "\n",
        "#user/game, rating\n",
        "gamerat = [[],[]]\n",
        "reviews = []\n",
        "\n",
        "erros = 0\n",
        "\n",
        "#aqui o vetor reviews sera acrescido de instancias.\n",
        "for i in vetorarqui:\n",
        "  with req.urlopen(cadaReview%i) as f:\n",
        "    data = f.read().decode('charmap')\n",
        "  data = data.replace('}\\n', '}-----')\n",
        "  data = data.split('-----')\n",
        "  print(\"reviews coletadas para esse jogo\",len(data))\n",
        "  for j in data:\n",
        "    try:\n",
        "      jfk = json.loads(j)\n",
        "      # infelizmente a base de dados contem instancias duplicadas\n",
        "      # Eventualmente um jogador comentou em dois jogos\n",
        "      # Eventualmente os jogadores mudam de nome, entao eh mais seguro filtrar pelo \"orig_url\"\n",
        "      #hum += 1\n",
        "      user = \"%s in %s\"%(jfk[\"orig_url\"], i[:-10])\n",
        "      revi = jfk[\"review\"]\n",
        "      revi = gensim.utils.simple_preprocess(revi)\n",
        "      rati = jfk[\"rating\"]\n",
        "      if (not (user in gamerat[0])) and len(revi) > 5 and len(revi) < 100:\n",
        "        gamerat[0].append(user)\n",
        "        reviews.append(revi)\n",
        "        gamerat[1].append(rati)\n",
        "    except:\n",
        "      erros += 1\n",
        "  print(reviews[-1])\n",
        "  print (\"Ha %i instancias\\nUm total de %i instancias ignoradas\"%(len(reviews),erros))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reviews coletadas para esse jogo 7274\n",
            "['simply', 'amazing', 'fun', 'to', 'play', 'insurgency', 'and', 'king', 'of', 'the', 'hill']\n",
            "Ha 880 instancias\n",
            "Um total de 135 instancias ignoradas\n",
            "reviews coletadas para esse jogo 6234\n",
            "['just', 'an', 'absolute', 'brilliant', 'game', 'you', 'can', 'play', 'on', 'any', 'server', 'you', 'want', 'and', 'you', 'will', 'have', 'lot', 'of', 'fun']\n",
            "Ha 2075 instancias\n",
            "Um total de 336 instancias ignoradas\n",
            "reviews coletadas para esse jogo 7412\n",
            "['you', 'shoot', 'chickens', 'in', 'this', 'game']\n",
            "Ha 2992 instancias\n",
            "Um total de 691 instancias ignoradas\n",
            "reviews coletadas para esse jogo 9792\n",
            "['yeah', 'best', 'game', 'cant', 'imagine', 'if', 'this', 'game', 'if', 'rlly', 'closed']\n",
            "Ha 4536 instancias\n",
            "Um total de 774 instancias ignoradas\n",
            "reviews coletadas para esse jogo 1532\n",
            "['ive', 'played', 'these', 'games', 'since', 'the', 'champ', 'man', 'days', 'this', 'has', 'to', 'be', 'the', 'worst', 'version', 'the', 'ai', 'of', 'the', 'game', 'is', 'unreal', 'you', 'have', 'players', 'hitting', 'the', 'post', 'countless', 'times', 'in', 'match', 'conceeding', 'goals', 'within', 'seconds', 'and', 'injury', 'lists', 'as', 'long', 'as', 'your', 'arm', 'these', 'are', 'all', 'part', 'of', 'the', 'game', 'in', 'real', 'life', 'you', 'may', 'say', 'agree', 'not', 'like', 'the', 'kind', 'you', 'see', 'in', 'this', 'game', 'you', 'can', 'be', 'barcelona', 'and', 'be', 'goal', 'down', 'within', 'seconds', 'with', 'messi', 'hitting', 'the', 'post', 'every', 'minutes', 'it', 'beats', 'the', 'keeper', 'the', 'keeper', 'gathers', 'the', 'ball']\n",
            "Ha 4716 instancias\n",
            "Um total de 785 instancias ignoradas\n",
            "reviews coletadas para esse jogo 7281\n",
            "['you', 'can', 'kill', 'someone', 'with', 'water', 'melon', 'and', 'then', 'drop', 'nuke', 'do', 'yourself', 'favor', 'and', 'get', 'this', 'game']\n",
            "Ha 5578 instancias\n",
            "Um total de 923 instancias ignoradas\n",
            "reviews coletadas para esse jogo 13853\n",
            "['best', 'version', 'of', 'gta', 'out', 'there', 'played', 'xbox', 'version', 'but', 'the', 'pc', 'version', 'up', 'the', 'xbox', 'in', 'so', 'many', 'ways', 'worth', 'it']\n",
            "Ha 7210 instancias\n",
            "Um total de 1463 instancias ignoradas\n",
            "reviews coletadas para esse jogo 7590\n",
            "['this', 'is', 'drugs', 'in', 'game', 'form']\n",
            "Ha 8116 instancias\n",
            "Um total de 1592 instancias ignoradas\n",
            "reviews coletadas para esse jogo 5760\n",
            "['the', 'game', 'is', 'amazing', 'other', 'poeple', 'in', 'the', 'game', 'are', 'usually', 'nice', 'and', 'fun', 'to', 'play', 'with', 'the', 'ommunity', 'for', 'this', 'game', 'is', 'huge', 'and', 'you', 'will', 'never', 'find', 'the', 'game', 'empty']\n",
            "Ha 8810 instancias\n",
            "Um total de 1684 instancias ignoradas\n",
            "reviews coletadas para esse jogo 7441\n",
            "['you', 'get', 'to', 'dragons', 'in', 'the', 'unprotected', 'then', 'you', 'absorb', 'their', 'essences', 'and', 'shout', 'at', 'giant', 'elephants', 'that', 'somehow', 'dont', 'give', 'flying', 'but', 'then', 'out', 'of', 'nowhere', 'giant', 'fat', 'black', 'guy', 'bangs', 'ur', 'head', 'with', 'his', 'floppy', 'so', 'hard', 'nasa', 'gets', 'jealous', 'of', 'how', 'high', 'you', 'go', 'wiz', 'khalifa', 'be', 'like', 'damn', 'he', 'high']\n",
            "Ha 9646 instancias\n",
            "Um total de 1977 instancias ignoradas\n",
            "reviews coletadas para esse jogo 7642\n",
            "['more', 'of', 'fan', 'of', 'first', 'person', 'shooter', 'and', 'also', 'the', 'mouse', 'movement', 'is', 'to', 'sensitive']\n",
            "Ha 10534 instancias\n",
            "Um total de 2523 instancias ignoradas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OxcNRE7OZSN",
        "colab_type": "text"
      },
      "source": [
        "## Criando o Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKe9poUcOe_D",
        "colab_type": "code",
        "outputId": "7767abf9-4f65-471d-f45f-1aac66e21978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dicionario = gensim.models.Word2Vec(\n",
        "        reviews,\n",
        "        size=128,\n",
        "        window=10,\n",
        "        min_count=2,\n",
        "        workers=100)\n",
        "dicionario.train(reviews, total_examples=len(reviews), epochs=20)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3957925, 5564120)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M8ThV2eRprN",
        "colab_type": "code",
        "outputId": "c2c55669-6dac-4abb-b952-65374cb768b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        }
      },
      "source": [
        "print(len(dicionario[\"good\"]))\n",
        "\n",
        "print(len(dicionario.wv.vocab))\n",
        "\n",
        "print(dicionario[\"good\"])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128\n",
            "6769\n",
            "[-1.7348461  -0.61153734  0.5915886  -0.04567001  0.63401294  0.836762\n",
            " -0.6689171   0.28501496 -2.8217402   1.5653055   0.48694682 -1.2471068\n",
            "  0.46384957 -0.46100026 -0.231443    1.1874955  -0.11163983 -1.0324322\n",
            "  0.7471781   0.51486355  0.8164231   1.2020351   1.4926608   0.5989936\n",
            " -0.7374438  -0.77841985  0.292716    1.8286482   0.4762717  -2.100407\n",
            " -1.5593988  -0.6398784   1.6421819  -0.28727528 -0.34670633  1.8824004\n",
            "  0.06329449  1.9996735   0.2833584   1.0744872  -1.5726855  -0.44091207\n",
            " -0.7978758   0.6843764  -0.03363505  0.53154266 -0.13131353 -1.0679569\n",
            " -1.7390774   2.066243    2.7412283  -1.5026373   0.77358645  0.17590383\n",
            " -0.26843593 -0.66805327 -0.7969052  -0.10585255  0.70911634  0.15260266\n",
            "  1.6899073  -2.0475569  -0.46596357 -0.13882676  0.7953005   0.7339992\n",
            "  0.04155761  1.002537   -0.9199573  -0.9764408  -1.442053   -0.53048384\n",
            "  0.01851841  0.80414116 -0.77925634  0.44386536  1.1213969   0.5598534\n",
            "  0.51858634  2.566204    1.5896723  -0.556941    0.7477198   0.20385277\n",
            " -0.39049128  1.7546979  -0.5734205  -1.6103572  -0.25417334  0.29783803\n",
            "  2.244568   -0.13343643  0.76939404 -0.14501992  0.6185512  -1.0742849\n",
            "  0.8005411   0.69823796 -1.3743565   1.4023409   0.06331076  0.12692943\n",
            "  1.4831392   0.9105203   1.3782336  -0.03745972 -0.4402406  -0.18200341\n",
            "  0.88580674 -0.9495753   0.01090322  0.5409582  -0.24539995  0.34307197\n",
            "  1.6283273  -1.721103    1.7431238   1.8239479  -0.9616472  -0.3868336\n",
            " -0.47959095 -0.0342363  -0.4018807   1.3019277  -0.64302284  1.9832494\n",
            "  0.3722683  -0.07742821]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOnVn0-YSf_I",
        "colab_type": "text"
      },
      "source": [
        "## Transformando cada review em uma matriz\n",
        "\n",
        "Como usado anteriormente, mas sem o bert, cada review sera transformada em uma matriz, onde cada linha é uma palavra com 128 valores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-8359RVTcGo",
        "colab_type": "code",
        "outputId": "abb192f4-dd20-4ce2-9b24-0dc9807feadd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "reviewsmat = []\n",
        "\n",
        "for i in reviews:\n",
        "  mm = []\n",
        "  for j in i:\n",
        "    try:\n",
        "      mm.append(dicionario[j])\n",
        "    except:\n",
        "      #print(\"Faltou\")\n",
        "      pass\n",
        "  mm = np.array(mm)\n",
        "  reviewsmat.append(mm)\n",
        "\n",
        "\n",
        "\n",
        "print(len(reviews))\n",
        "print(len(reviewsmat))\n",
        "print(len(reviewsmat[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10534\n",
            "10534\n",
            "67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyYI2A6OWPh_",
        "colab_type": "code",
        "outputId": "b7fe121b-c33f-448c-fe42-aa238bb3db66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#pading, para tudo ter o mesmo tamanho\n",
        "\n",
        "maxlen = 0\n",
        "\n",
        "revFiltradas = []\n",
        "\n",
        "for i in reviewsmat:\n",
        "  if i.shape[0] > maxlen:\n",
        "    maxlen = i.shape[0]\n",
        "\n",
        "erros = 0\n",
        "\n",
        "for i in range(len(reviewsmat)):\n",
        "  try:\n",
        "    pad = np.zeros((maxlen, 128))\n",
        "    pad[:reviewsmat[i].shape[0],:] = reviewsmat[i]\n",
        "    revFiltradas.append(pad)\n",
        "  except:\n",
        "    erros += 1\n",
        "\n",
        "print(maxlen, erros)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEdgHZdVbQLh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d4b7ce4-e777-4981-fb5f-f21607ffabd9"
      },
      "source": [
        "#Reshaping\n",
        "\n",
        "inst = len(revFiltradas)\n",
        "\n",
        "x = np.array(revFiltradas)\n",
        "\n",
        "x = x.reshape(inst, maxlen, 128, 1)\n",
        "\n",
        "len(x)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10534"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DYOqRZRfePV",
        "colab_type": "text"
      },
      "source": [
        "## One hot encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gglvz7jidYKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "outputId": "fc56cd49-f23e-4c66-8fcc-b47f85defbd6"
      },
      "source": [
        "#Outputs\n",
        "\n",
        "ardata = np.array(gamerat[1])\n",
        "print(ardata)\n",
        "\n",
        "\n",
        "# https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(ardata)\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)\n",
        "# invert first example\n",
        "inverted = label_encoder.inverse_transform([np.argmax(onehot_encoded[0, :])])\n",
        "print(inverted)\n",
        "\n",
        "\n",
        "y = keras.utils.to_categorical(onehot_encoded, 2)\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Recommended' 'Recommended' 'Recommended' ... 'Recommended' 'Recommended'\n",
            " 'Not Recommended']\n",
            "[1 1 1 ... 1 1 0]\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "['Recommended']\n",
            "[[[1. 0.]\n",
            "  [0. 1.]]\n",
            "\n",
            " [[1. 0.]\n",
            "  [0. 1.]]\n",
            "\n",
            " [[1. 0.]\n",
            "  [0. 1.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0.]\n",
            "  [0. 1.]]\n",
            "\n",
            " [[1. 0.]\n",
            "  [0. 1.]]\n",
            "\n",
            " [[0. 1.]\n",
            "  [1. 0.]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLepzjVqg7Ay",
        "colab_type": "text"
      },
      "source": [
        "## Classificador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1dS_1ethDcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK4CtNKfAFwv",
        "colab_type": "text"
      },
      "source": [
        "## Referencias\n",
        "\n"
      ]
    }
  ]
}