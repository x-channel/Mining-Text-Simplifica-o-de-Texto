{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projeto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/projeto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP583IXGNeHC",
        "colab_type": "text"
      },
      "source": [
        "# Projeto de Simplificação de Texto\n",
        "\n",
        "Nas primeiras abordagens, houve uma tentativa de produzir uma rede neural que tivesse como entrada o *token frequence* e saida outro *token frequence*.\n",
        " Apesar disso poder ser considerado um resultado válido para a tentativa de melhorar a classificação de texto, não podemos considerar uma frase como um conjunto de palavras com a propriedade da comutatividade.\n",
        "  A ordem das palavras acaba importando muito para os humanos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptDnWb-pbJUb",
        "colab_type": "text"
      },
      "source": [
        "## importando bugingangas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh5apxspbJ0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import random\n",
        "#import pandas as pd\n",
        "from urllib import request as req\n",
        "from gensim.models import keyedvectors as kv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQFno4IJOzW3",
        "colab_type": "text"
      },
      "source": [
        "## Paper With Code\n",
        "\n",
        "~~Após um gole de sorte, eu acabo por encontrar essa maravilha~~ chamado de [Paper With Code](https://paperswithcode.com/sota/document-summarization-on-cnn-daily-mail), é um ~~bom~~ compilado de resultados cientificos sobre determinados problemas da computação. O primeiro artigo do link de cima, mostra uma solução com ROGUE-1 de 43.83 para o problema de sumarização de documentos como o *state of the art*.\n",
        "\n",
        "![paper with code](https://github.com/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/imagens/paperwithcode.png?raw=true)\n",
        "<center>paper with code</center>\n",
        "\n",
        "No artigo *Text Summarization with Pretrained Encoders* (LIU e LAPATA, 2019), os autores representam o texto como *Bert*, que é gerado por uma rede neural e é um caso especial do *word2vec*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzGa9dI5viX",
        "colab_type": "text"
      },
      "source": [
        "## Representação do Bert word2vec\n",
        "\n",
        "*Kyubyong* criou um preset do Bert, um dicionário onde cada palavra é representada por um vetor de 768 valores decimais.\n",
        "Visto algumas limitações das plataformas *Google Colab* e do *Github*, serão usadas apenas 300 instâncias do CNN, que gerou um dicionário reduzido de apenas 25mb.\n",
        "\n",
        "![Bert is Evil](https://github.com/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/imagens/audio-banner.jpg?raw=true)\n",
        "<center>Bert is Evil</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iap7h5R65zWn",
        "colab_type": "text"
      },
      "source": [
        "## Base de dados da Cable News Network\n",
        "\n",
        "A base de dados pode ser encontrado [nesse github da google](https://github.com/google-research-datasets/sentence-compression), no formato *Json*. Essa base de dados é formada com notícias da CNN com a primeira linha da noticia, com o título, com todos os bigramas possíveis e com informações do TAG. Porém no trabalho apenas será usado a primeira linha como entrada e o título como saída.\n",
        "\n",
        "![Json](https://github.com/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/imagens/Jasonf.jpg?raw=true)\n",
        "<center>Json</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Nkg503WqUi",
        "colab_type": "text"
      },
      "source": [
        "## Mesclar o bert com o json.\n",
        "\n",
        "Basicamente esse script carrega o vocabulário do dicionário bert, onde a partir da segunda linha, cada linha é uma palavra, seguida de sua representação vetorial com 768 valores.\n",
        "Eventualmente aparecem algumas palavras estranhas como \"##,\".\n",
        "Essa palavra significa somente que a palavra anterior termina com uma virgula.\n",
        "\n",
        "Depois de carregar o vocabulário, ele começa a ler as noticias do json.\n",
        "Ele segmenta uma noticia, a transformando em um dicionário do python, procura os textos alvos\n",
        "### jfk['graph']['sentence'] e jfk['headline'].\n",
        "\n",
        "Com os textos em mãos, ele cata as palavras de cada texto, criando uma matriz de tamanho 768*2 pela quantidade de linhas do subtitulo da noticia.\n",
        "\n",
        "Observar que para esse código rodar ele deve estar na seguinte pasta ~~por isso está comentado~~. Com o json nomeado *in.json*, com o bert nomeado *dicionariolongo.vec* e uma pasta vazia chamada de *instancias*. O script vai gerar um *dicionario.csv* e varias noticias dentro da pasta instancias. Infelizmente algumas noticias tem caracters que não permitem seu uso como nome do arquivo, por isso todas as noticias foram nomeadas de *arquivo_n.csv* ~~Outro motivo para você guardar seus dados em um único arquivo json~~.\n",
        "\n",
        ">O diretório do arquivo.\n",
        ">>in.json \\n\n",
        "dicionariolongo.vec \\n\n",
        "preprobert.py\n",
        ">>> instancias\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Uesl-rWdP1",
        "colab_type": "code",
        "outputId": "245dde8b-4dd6-458c-b7f3-845ade14846f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "#texto para matriz Bert\n",
        "\n",
        "#!/usr/bin/python\n",
        "# -*- coding: UTF-8 -*-\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import fileinput\n",
        "\n",
        "import codecs\n",
        "\n",
        "import os\n",
        "\n",
        "dici = {}\n",
        "fins = {}\n",
        "dico = {} #dicionario reduzido\n",
        "\n",
        "\n",
        "#carrega o dicionario bert\n",
        "with open(\"dicionariolongo.vec\", \"r\", encoding=\"utf-8\") as d:\n",
        "    for i in d:\n",
        "        if len(i) > 200: #isso aqui eh para tirar a primeira linha, altura x largura\n",
        "            try:\n",
        "                a = i.split()\n",
        "                if '##' in a[0]:\n",
        "                    fins[a[0][2:].lower()] = []\n",
        "                    for j in a[1:]:\n",
        "                        fins[a[0][2:].lower()].append(float(j))\n",
        "                elif (not a[0].lower() in dici) or a[0].islower():\n",
        "                    dici[a[0].lower()] = []\n",
        "                    for j in a[1:]:\n",
        "                        dici[a[0].lower()].append(float(j))\n",
        "            except ValueError:\n",
        "                print(\"problema com uma palavra\")\n",
        "\n",
        "\n",
        "\n",
        "with open(\"in.json\", 'rb') as f:\n",
        "    data = f.read()\n",
        "\n",
        "data = str(data)[2:-1]\n",
        "data = data.replace('\\\\n}', '\\\\n}-----')\n",
        "data = data.split('-----')\n",
        "\n",
        "blanckl = []\n",
        "for i in range(768):\n",
        "    blanckl.append('')\n",
        "\n",
        "instancias = 0\n",
        "for p in data[0:-1]:\n",
        "    entrada = []\n",
        "    saida = []\n",
        "    try:\n",
        "        jfk = json.loads(codecs.decode(p, 'unicode_escape'))\n",
        "        arquivo = jfk['graph']['sentence'].lower().replace(\"/\",\"\").replace(\"\\\\\",\"\")\n",
        "        lg = jfk['graph']['sentence'].lower().split()\n",
        "        st = jfk['headline'].lower().split()\n",
        "\n",
        "        for i in lg: #isso deveria ser uma funcao, addlist() #semTempo\n",
        "            if i in dico: #dicionario reduzido\n",
        "                entrada.append(dico[i])\n",
        "            elif i in dici:\n",
        "                dico[i] = dici[i]\n",
        "                entrada.append(dico[i])\n",
        "            else:\n",
        "                for j, jj in fins.items():\n",
        "                    if j in i[-len(j):]:\n",
        "                        i = i[:-len(j)]\n",
        "                        if i in dico: #assim ela poderia ser chamada aqui\n",
        "                            entrada.append(dico[i])\n",
        "                        elif i in dici:\n",
        "                            dico[i] = dici[i]\n",
        "                            entrada.append(dico[i])\n",
        "                        if j in dico:\n",
        "                            entrada.append(dico[j])\n",
        "                        else:\n",
        "                            dico[j] = fins[j]\n",
        "                            entrada.append(dico[j])\n",
        "                        break\n",
        "        for i in st: #isso deveria ser uma funcao, addlist() #semTempo\n",
        "            if i in dico: #dicionario reduzido\n",
        "                saida.append(dico[i])\n",
        "            elif i in dici:\n",
        "                dico[i] = dici[i]\n",
        "                saida.append(dico[i])\n",
        "            else:\n",
        "                for j, jj in fins.items():\n",
        "                    if j in i[-len(j):]:\n",
        "                        i = i[:-len(j)]\n",
        "                        if i in dico: #assim ela poderia ser chamada aqui\n",
        "                            saida.append(dico[i])\n",
        "                        elif i in dici:\n",
        "                            dico[i] = dici[i]\n",
        "                            saida.append(dico[i])\n",
        "                        if j in dico:\n",
        "                            saida.append(dico[j])\n",
        "                        else:\n",
        "                            dico[j] = fins[j]\n",
        "                            saida.append(dico[j])\n",
        "                        break\n",
        "    except ValueError:\n",
        "        print(\"Houve um Erro\")\n",
        "\n",
        "        \n",
        "    sv = os.getcwd() + '\\\\instancias\\\\arquivo_%i'%instancias + '.csv'\n",
        "    instancias += 1\n",
        "    try:\n",
        "        with open(sv, 'w', newline = '') as file:\n",
        "            writer = csv.writer(file)\n",
        "            tam = 0\n",
        "            while tam < len(entrada):\n",
        "                tm = []\n",
        "                #print(type(tm))\n",
        "                tm.extend(entrada[tam])\n",
        "                if tam < len(saida):\n",
        "                    tm.extend(saida[tam])\n",
        "                else:\n",
        "                    tm.extend(blanckl)\n",
        "                tam += 1\n",
        "                writer.writerow(tm)\n",
        "    except ValueError:\n",
        "        print(\"erro no json\")\n",
        "\n",
        "with open(\"dicionario.csv\", \"w\", newline = '') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for i, ii in dico.items():\n",
        "\ttm = []\n",
        "\ttm.append(i)\n",
        "\ttm.extend(ii)\n",
        "        writer.writerow(tm)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#texto para matriz Bert\\n\\n#!/usr/bin/python\\n# -*- coding: UTF-8 -*-\\n\\nimport pandas as pd\\nimport csv\\n\\nimport nltk\\nimport numpy as np\\n\\nimport json\\nimport fileinput\\n\\nimport codecs\\n\\nimport os\\n\\ndici = {}\\nfins = {}\\ndico = {} #dicionario reduzido\\n\\n\\n#carrega o dicionario bert\\nwith open(\"dicionariolongo.vec\", \"r\", encoding=\"utf-8\") as d:\\n    for i in d:\\n        if len(i) > 200: #isso aqui eh para tirar a primeira linha, altura x largura\\n            try:\\n                a = i.split()\\n                if \\'##\\' in a[0]:\\n                    fins[a[0][2:].lower()] = []\\n                    for j in a[1:]:\\n                        fins[a[0][2:].lower()].append(float(j))\\n                elif (not a[0].lower() in dici) or a[0].islower():\\n                    dici[a[0].lower()] = []\\n                    for j in a[1:]:\\n                        dici[a[0].lower()].append(float(j))\\n            except ValueError:\\n                print(\"problema com uma palavra\")\\n\\n\\n\\nwith open(\"in.json\", \\'rb\\') as f:\\n    data = f.read()\\n\\ndata = str(data)[2:-1]\\ndata = data.replace(\\'\\\\n}\\', \\'\\\\n}-----\\')\\ndata = data.split(\\'-----\\')\\n\\nblanckl = []\\nfor i in range(768):\\n    blanckl.append(\\'\\')\\n\\ninstancias = 0\\nfor p in data[0:-1]:\\n    entrada = []\\n    saida = []\\n    try:\\n        jfk = json.loads(codecs.decode(p, \\'unicode_escape\\'))\\n        arquivo = jfk[\\'graph\\'][\\'sentence\\'].lower().replace(\"/\",\"\").replace(\"\\\\\",\"\")\\n        lg = jfk[\\'graph\\'][\\'sentence\\'].lower().split()\\n        st = jfk[\\'headline\\'].lower().split()\\n\\n        for i in lg: #isso deveria ser uma funcao, addlist() #semTempo\\n            if i in dico: #dicionario reduzido\\n                entrada.append(dico[i])\\n            elif i in dici:\\n                dico[i] = dici[i]\\n                entrada.append(dico[i])\\n            else:\\n                for j, jj in fins.items():\\n                    if j in i[-len(j):]:\\n                        i = i[:-len(j)]\\n                        if i in dico: #assim ela poderia ser chamada aqui\\n                            entrada.append(dico[i])\\n                        elif i in dici:\\n                            dico[i] = dici[i]\\n                            entrada.append(dico[i])\\n                        if j in dico:\\n                            entrada.append(dico[j])\\n                        else:\\n                            dico[j] = fins[j]\\n                            entrada.append(dico[j])\\n                        break\\n        for i in st: #isso deveria ser uma funcao, addlist() #semTempo\\n            if i in dico: #dicionario reduzido\\n                saida.append(dico[i])\\n            elif i in dici:\\n                dico[i] = dici[i]\\n                saida.append(dico[i])\\n            else:\\n                for j, jj in fins.items():\\n                    if j in i[-len(j):]:\\n                        i = i[:-len(j)]\\n                        if i in dico: #assim ela poderia ser chamada aqui\\n                            saida.append(dico[i])\\n                        elif i in dici:\\n                            dico[i] = dici[i]\\n                            saida.append(dico[i])\\n                        if j in dico:\\n                            saida.append(dico[j])\\n                        else:\\n                            dico[j] = fins[j]\\n                            saida.append(dico[j])\\n                        break\\n    except ValueError:\\n        print(\"Houve um Erro\")\\n\\n        \\n    sv = os.getcwd() + \\'\\\\instancias\\\\arquivo_%i\\'%instancias + \\'.csv\\'\\n    instancias += 1\\n    try:\\n        with open(sv, \\'w\\', newline = \\'\\') as file:\\n            writer = csv.writer(file)\\n            tam = 0\\n            while tam < len(entrada):\\n                tm = []\\n                #print(type(tm))\\n                tm.extend(entrada[tam])\\n                if tam < len(saida):\\n                    tm.extend(saida[tam])\\n                else:\\n                    tm.extend(blanckl)\\n                tam += 1\\n                writer.writerow(tm)\\n    except ValueError:\\n        print(\"erro no json\")\\n\\nwith open(\"dicionario.csv\", \"w\", newline = \\'\\') as file:\\n    writer = csv.writer(file)\\n    for i, ii in dico.items():\\n\\ttm = []\\n\\ttm.append(i)\\n\\ttm.extend(ii)\\n        writer.writerow(tm)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKjJH3pbaixJ",
        "colab_type": "text"
      },
      "source": [
        "## Abrindo uma noticia (bert) e convertendo para texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6WEsqOPajg1",
        "colab_type": "code",
        "outputId": "97df4f06-7289-4931-c164-2e475d9255b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "# Abrindo o dicionario criado com o outro script\n",
        "dicurl = 'https://raw.githubusercontent.com/x-channel/Mining-Text-Simplifica-o-de-Texto/master/dataset/dicionario.csv'\n",
        "#dicloader = req.urlopen(dicurl)\n",
        "vetores = []\n",
        "palavras = []\n",
        "\n",
        "with req.urlopen(dicurl) as f:\n",
        "  meucsv = f.read().decode('charmap')\n",
        "  meucsv = meucsv.split('\\n')[:-1]\n",
        "  for i in meucsv:\n",
        "    j = i.replace('\\r','').split(',')\n",
        "    palavras.append(j[0])\n",
        "    vetores.append(j[1:])\n",
        "\n",
        "# colocando o dicionario no gensim.\n",
        "dicio = kv.Word2VecKeyedVectors(768)\n",
        "dicio.add(palavras, np.array(vetores).astype(float))\n",
        "\n",
        "#busca por um vetor parecido\n",
        "five = dicio.get_vector('five')\n",
        "friends = dicio.get_vector('friends')\n",
        "\n",
        "print(dicio.similar_by_vector(five, 1), dicio.similar_by_vector(friends, 1))\n",
        "\n",
        "#abrindo a noticia\n",
        "#nn = input(\"digite um numero entre 0 e 199: \")\n",
        "nn = 1\n",
        "noticia = 'https://raw.githubusercontent.com/x-channel/Mining-Text-Simplifica-o-de-Texto/master/dataset/noticias/arquivo_%i.csv'%nn\n",
        "\n",
        "pastaNoticias = 'https://raw.githubusercontent.com/x-channel/Mining-Text-Simplifica-o-de-Texto/master/dataset/noticias/%s_%i.csv'\n",
        "\n",
        "def abrirNoticia(urlnoticia):\n",
        "  sentence = []\n",
        "  head = []\n",
        "  with req.urlopen(urlnoticia) as f:\n",
        "    meucsv = f.read().decode('charmap')\n",
        "    meucsv = meucsv.split('\\n')[:-1]\n",
        "    for i in meucsv:\n",
        "      j = i.replace('\\r',',').replace(',,', ',0.0,').replace(',,', ',0.0,')[:-1]\n",
        "      j = j.split(',')\n",
        "      sentence.append(j[:768])\n",
        "      head.append(j[768:])\n",
        "  sentence = np.array(sentence).astype(float)\n",
        "  head = np.array(head).astype(float)\n",
        "  return sentence, head\n",
        "\n",
        "def abrirNoticias(urlfolder, total, nome = 'arquivo'):\n",
        "  primeiro = []\n",
        "  cabecalh = []\n",
        "  for i in range(total):\n",
        "    par = abrirNoticia(urlfolder%(nome,i))\n",
        "    primeiro.append(par[0])\n",
        "    cabecalh.append(par[1])\n",
        "  return np.array(primeiro),np.array(cabecalh)\n",
        "\n",
        "\n",
        "def vec2head(matriz, model, li):\n",
        "  head = []\n",
        "  for i in matriz:\n",
        "    j = model.similar_by_vector(i,1)\n",
        "    if j[0][1] > li:\n",
        "      head.append(j)\n",
        "  return head\n",
        "\n",
        "s, h = abrirNoticia(noticia)\n",
        "\n",
        "titulo = vec2head(h, dicio, 0.5)\n",
        "print (titulo)\n",
        "\n",
        "noticias,cabecalhos = abrirNoticias(pastaNoticias, 200)\n",
        "print(vec2head(noticias[1], dicio, 0.5))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('five', 1.0)] [('friends', 1.0)]\n",
            "[[('several', 1.0)], [('school', 1.0)], [('districts', 1.0)], [('hold', 1.0)], [('classes', 1.0)], [('on', 1.0)], [('president', 0.6351367831230164)], [(\"##'\", 0.9999998807907104)], [('day', 1.0000001192092896)], [('to', 1.0)], [('make', 1.0)], [('up', 1.0000001192092896)], [('for', 1.0)], [('days', 1.0)], [('missed', 1.0)]]\n",
            "[[('several', 1.0)], [('school', 1.0)], [('districts', 1.0)], [('in', 1.0)], [('hampton', 1.0)], [('roads', 1.0)], [('are', 0.9999999403953552)], [('holding', 1.0)], [('classes', 1.0)], [('this', 1.0000001192092896)], [('president', 0.6351367831230164)], [(\"##'\", 0.9999998807907104)], [('day', 1.0000001192092896)], [('to', 1.0)], [('make', 1.0)], [('up', 1.0000001192092896)], [('for', 1.0)], [('days', 1.0)], [('missed', 1.0)], [('because', 1.0)], [('of', 1.0)], [('the', 1.0)], [(\"##'\", 0.696808934211731)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNJpRDlg4kQW",
        "colab_type": "text"
      },
      "source": [
        "## Dividindo a base de dados\n",
        "\n",
        "Aqui a base de dados é dividida em: Treinamento, validação e teste.\n",
        "\n",
        "OBS: não encontrei isso implementado nem no scikit learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9psE79f5CmX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "b0356894-8991-44c1-d2a0-38aabc963477"
      },
      "source": [
        "treinamento = []\n",
        "#validacao = []\n",
        "teste = []\n",
        "\n",
        "xt = []\n",
        "yt = []\n",
        "\n",
        "xr = []\n",
        "yr = []\n",
        "\n",
        "atreino = 0.9\n",
        "#avalidacao = 0.3\n",
        "\n",
        "d = []\n",
        "for i in range(len(noticias)):\n",
        "  d.append(i)\n",
        "\n",
        "random.shuffle(d)\n",
        "\n",
        "# deixar essa linha para validar os parametros dos primeiros testes\n",
        "d = [117, 135, 181, 2, 129, 167, 65, 183, 107, 104, 158, 111, 69, 194, 8, 101, 21, 35, 31, 188, 106, 196, 148, 198, 67, 60, 102, 82, 16, 88, 119, 61, 11, 115, 113, 56, 169, 98, 64, 40, 49, 162, 36, 127, 157, 66, 164, 180, 41, 138, 62, 34, 72, 178, 27, 189, 121, 154, 96, 14, 133, 145, 97, 43, 199, 51, 25, 163, 155, 47, 70, 150, 12, 30, 123, 195, 32, 55, 18, 176, 171, 68, 175, 120, 110, 59, 141, 6, 23, 44, 103, 151, 125, 130, 79, 73, 173, 1, 58, 165, 118, 46, 39, 191, 10, 74, 166, 24, 147, 131, 190, 20, 156, 26, 22, 187, 182, 75, 63, 52, 9, 132, 87, 5, 144, 192, 42, 142, 90, 85, 143, 13, 153, 174, 122, 139, 184, 128, 19, 50, 161, 172, 168, 83, 48, 71, 185, 53, 126, 4, 29, 86, 15, 7, 92, 45, 197, 76, 134, 37, 54, 152, 57, 84, 112, 3, 28, 93, 0, 109, 136, 177, 77, 170, 100, 146, 137, 179, 80, 33, 17, 124, 89, 193, 38, 160, 78, 95, 140, 114, 159, 81, 186, 99, 108, 105, 116, 94, 149, 91]\n",
        "\n",
        "for i in range(len(noticias)):\n",
        "  if i < atreino*len(noticias):\n",
        "    xt.append(noticias[d[i]])\n",
        "    yt.append(cabecalhos[d[i]])\n",
        "  else:\n",
        "    xr.append(noticias[d[i]])\n",
        "    yr.append(cabecalhos[d[i]])\n",
        "\n",
        "xt = np.array(xt)\n",
        "yt = np.array(yt)\n",
        "\n",
        "xr = np.array(xr)\n",
        "yr = np.array(yr)\n",
        "\n",
        "print(len(xt), len(yt))\n",
        "print(len(xr), len(xr[1]), len(xr[1][1]))\n",
        "\n",
        "'''\n",
        "for i in range(len(noticias)):\n",
        "  if i < atreino*len(noticias):\n",
        "    treinamento.append(noticias[d[i]])\n",
        "  elif i < (atreino+avaliacao)*len(noticias):\n",
        "    validacao.append(noticias[d[i]])\n",
        "  else:\n",
        "    teste.append(noticias[d[i]])'''\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "180 180\n",
            "20 21 768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor i in range(len(noticias)):\\n  if i < atreino*len(noticias):\\n    treinamento.append(noticias[d[i]])\\n  elif i < (atreino+avaliacao)*len(noticias):\\n    validacao.append(noticias[d[i]])\\n  else:\\n    teste.append(noticias[d[i]])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7aMectc4YIn",
        "colab_type": "text"
      },
      "source": [
        "## Construindo a Rede Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thAFO7kN4eFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnC1bMvV4eme",
        "colab_type": "text"
      },
      "source": [
        "## Treinando a rede"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyMsp1u95Jo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXYJKsJ5ckLT",
        "colab_type": "text"
      },
      "source": [
        "##Teste da rede\n",
        "\n",
        "Aqui a rede é testada, sem produzir de fato saidas legíveis, mas apenas mostrando uma acurácia da rede em relação ao vetor bert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QS0KNZscwX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZGIJBRi5J7m",
        "colab_type": "text"
      },
      "source": [
        "## Teste de predição\n",
        "\n",
        "Aqui a rede é rodada como teste. A metrica mais usada para esse teste é o ROUGE-1, que conta quantas palavras da saída do sistema está dentro do texto de referência, depois divide pelo número de palavras no texto de referência."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFw4uRsm5Kuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#rouge-1\n",
        "def rouge(src, ref):\n",
        "  acertos = 0\n",
        "  for i in src:\n",
        "    pass\n",
        "  return acertos/float(len(ref))\n",
        "\n",
        "sistem = ['five', 'plane']\n",
        "refere = ['five', 'in', 'plane']\n",
        "\n",
        "print(rouge(sistem, refere))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK4CtNKfAFwv",
        "colab_type": "text"
      },
      "source": [
        "## Referencias\n",
        "\n"
      ]
    }
  ]
}