{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projeto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/projeto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP583IXGNeHC",
        "colab_type": "text"
      },
      "source": [
        "# Projeto de Simplificação de Texto\n",
        "\n",
        "Nas primeiras abordagens, houve uma tentativa de produzir uma rede neural que tivesse como entrada o *token frequence* e saida outro *token frequence*. Apesar disso poder ser considerado um resultado válido para a tentativa de melhorar a classificação de texto, não podemos considerar uma frase como um conjunto de palavras com a propriedade da comutatividade. A ordem das palavras acaba importando muito para os humanos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptDnWb-pbJUb",
        "colab_type": "text"
      },
      "source": [
        "## importando bugingangas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh5apxspbJ0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import numpy as np\n",
        "import csv\n",
        "#import pandas as pd\n",
        "from urllib import request as req\n",
        "from gensim.models import keyedvectors as kv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQFno4IJOzW3",
        "colab_type": "text"
      },
      "source": [
        "## Paper With Code\n",
        "\n",
        "~~Após um gole de sorte, eu acabo por encontrar essa maravilha~~ chamado de [Paper With Code](https://paperswithcode.com/sota/document-summarization-on-cnn-daily-mail), é um ~~bom~~ compilado de resultados cientificos sobre determinados problemas da computação. O primeiro artigo do link de cima, mostra uma solução com ROGUE-1 de 43.83 para o problema de sumarização de documentos como o *state of the art*.\n",
        "\n",
        "![paper with code](https://github.com/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/imagens/paperwithcode.png?raw=true)\n",
        "<center>paper with code</center>\n",
        "\n",
        "No artigo *Text Summarization with Pretrained Encoders* (LIU e LAPATA, 2019), os autores representam o texto como *Bert*, que é gerado por uma rede neural e é um caso especial do *word2vec*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npzGa9dI5viX",
        "colab_type": "text"
      },
      "source": [
        "## Representação do Bert word2vec\n",
        "\n",
        "*Kyubyong* criou um preset do Bert, um dicionário onde cada palavra é representada por um vetor de 768 valores decimais. Visto algumas limitações das plataformas *Google Colab* e do *Github*, serão usadas apenas 300 instâncias do CNN, que gerou um dicionário reduzido de apenas 25mb.\n",
        "\n",
        "![Bert is Evil](https://github.com/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/imagens/audio-banner.jpg?raw=true)\n",
        "<center>Bert is Evil</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iap7h5R65zWn",
        "colab_type": "text"
      },
      "source": [
        "## Base de dados da Cable News Network\n",
        "\n",
        "A base de dados pode ser encontrado [nesse github da google](https://github.com/google-research-datasets/sentence-compression), no formato *Json*. Essa base de dados é formada com notícias da CNN com a primeira linha da noticia, com o título, com todos os bigramas possíveis e com informações do TAG. Porém no trabalho apenas será usado a primeira linha como entrada e o título como saída.\n",
        "\n",
        "![Json](https://github.com/x-channel/Mining-Text-Simplifica-o-de-Texto/blob/master/imagens/Jasonf.jpg?raw=true)\n",
        "<center>Json</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKjJH3pbaixJ",
        "colab_type": "text"
      },
      "source": [
        "## Abrindo uma noticia (bert) e convertendo para texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6WEsqOPajg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Abrindo o dicionario\n",
        "dicurl = 'https://raw.githubusercontent.com/x-channel/Mining-Text-Simplifica-o-de-Texto/master/dataset/dicionario.csv'\n",
        "#dicloader = req.urlopen(dicurl)\n",
        "dicio = []\n",
        "\n",
        "with req.urlopen(dicurl) as f:\n",
        "  meucsv = f.read().decode('charmap')\n",
        "  meucsv = meucsv.split('\\n')\n",
        "  for i in meucsv:\n",
        "    dicio.append(i)\n",
        "\n",
        "print(dicio[0])\n",
        "\n",
        "#abrindo a noticia\n",
        "#nn = input(\"digite um numero entre 0 e 199: \")\n",
        "nn = 1\n",
        "urlnoticia = 'https://raw.githubusercontent.com/x-channel/Mining-Text-Simplifica-o-de-Texto/master/dataset/noticias/arquivo_%i.csv'%nn\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK4CtNKfAFwv",
        "colab_type": "text"
      },
      "source": [
        "## Referencias\n",
        "\n"
      ]
    }
  ]
}